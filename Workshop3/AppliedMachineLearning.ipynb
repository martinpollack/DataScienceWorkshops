{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JPYaq5eWQ4n"
      },
      "source": [
        "# Applied Machine Learning\n",
        "### Workshop 3 of DASIL's series on \"Data Science with Python\"\n",
        "### Created by Martin Pollack, Yusen He, and Declan O'Reilly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FGuCgcWQ4p"
      },
      "source": [
        "In this Jupyter notebook we will teach you how to fit the machine learning models we talked about last week in Python using the `scikit-learn` package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B4_h-5j_Wqc"
      },
      "source": [
        "All of our example datasets come from the `datasets` sub-package within `scikit-learn`. So we import them now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UiT01kfZWQ4q"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEOwnQpUWQ4t"
      },
      "source": [
        "## Supervised Learning - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHTB5d26lQO"
      },
      "source": [
        "#### Dataset introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7L_N9Q6WQ4u"
      },
      "source": [
        "Remember that in a regression problem the outcome variable is numeric and continuous. However, the predictor variables can either be continuous or discrete.\n",
        "\n",
        "An example of a regression problem can be found in the diabetes dataset within sklearn. Our outcome is a quantitative measure of disease progression that takes on numbers between 25 and 346."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ffUxHDLWQ4v"
      },
      "outputs": [],
      "source": [
        "diabetes = datasets.load_diabetes(as_frame=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuLcLCoQ_Wqk"
      },
      "source": [
        "`diabetes` is a special scikit-learn dataset. Its `target` field is a pandas Series with our continuous outcome variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAset6_s_Wqm",
        "outputId": "da8333dd-df5a-43af-c2c2-e3ac331fc008"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      151.0\n",
              "1       75.0\n",
              "2      141.0\n",
              "3      206.0\n",
              "4      135.0\n",
              "       ...  \n",
              "437    178.0\n",
              "438    104.0\n",
              "439    132.0\n",
              "440    220.0\n",
              "441     57.0\n",
              "Name: target, Length: 442, dtype: float64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrEfXZuATy5"
      },
      "source": [
        "And its `data` field is a pandas Dataframe of all of our potential predictor variables, which are:\n",
        "*   **age:** age in years\n",
        "*   **sex:** gender\n",
        "*   **bmi:** body mass index\n",
        "*   **bp:** average blood pressure\n",
        "*   **s1 tc:** total serum cholesterol\n",
        "*   **s2 ldl:** low-density lipoproteins\n",
        "*   **s3 hdl:** high-density lipoproteins\n",
        "*   **s4 tch:** total cholesterol / HDL\n",
        "*   **s5 ltg:** possibly log of serum triglycerides level\n",
        "*   **s6 glu:** blood sugar level\n",
        "\n",
        "Note that all 10 variables inside `diabetes` dataset have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uZND0mNpYxjS",
        "outputId": "3e11ac00-0ee1-4156-d387-0e25dae51ca3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>0.044485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017282</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.015491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.003064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age       sex       bmi        bp        s1        s2        s3  \\\n",
              "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
              "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
              "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
              "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
              "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
              "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
              "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
              "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
              "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
              "\n",
              "           s4        s5        s6  \n",
              "0   -0.002592  0.019908 -0.017646  \n",
              "1   -0.039493 -0.068330 -0.092204  \n",
              "2   -0.002592  0.002864 -0.025930  \n",
              "3    0.034309  0.022692 -0.009362  \n",
              "4   -0.002592 -0.031991 -0.046641  \n",
              "..        ...       ...       ...  \n",
              "437 -0.002592  0.031193  0.007207  \n",
              "438  0.034309 -0.018118  0.044485  \n",
              "439 -0.011080 -0.046879  0.015491  \n",
              "440  0.026560  0.044528 -0.025930  \n",
              "441 -0.039493 -0.004220  0.003064  \n",
              "\n",
              "[442 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLh59zKN_Wqr"
      },
      "source": [
        "To make things easier, we will just rename our `target` to `Y` and our predictors to `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "mvjfRKLu_Wqs"
      },
      "outputs": [],
      "source": [
        "Y = diabetes.target\n",
        "X = diabetes.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amRvAyBZaNez"
      },
      "source": [
        "#### Splitting the dataset into Training and Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ro8jKDFpwSh"
      },
      "source": [
        "To evaluate the performance of our models, we need to randomly split our features dataset `X` and outcomes `Y` into a training set `X_train`/`y_train` and testing set `X_test`/`Y_test`. Randomly, we will use some observations to train our models and others to see how our model generalizes to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40iJCmxjcLVC"
      },
      "source": [
        "The `sklearn.model_selection.train_test_split()` does the random split for the training and testing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VADJGmpq-96"
      },
      "source": [
        "The `test_size` option controls proportion of the observations in our original data that we want to include in our test dataset. The `random_state` parameter controls the random selection process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "NZBrjZyeaTxa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ3sh-bKq0ko",
        "outputId": "e4d4c338-29cb-4256-d0f0-ca676fe584d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of total input dataset is: (442, 10)\n",
            "The shape of training input is: (331, 10)\n",
            "The shape of test input is: (111, 10)\n",
            "The shape of total output is: (442,)\n",
            "The shape of training output is: (331,)\n",
            "The shape of test output is: (111,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The shape of total input dataset is:\",X.shape)\n",
        "print(\"The shape of training input is:\",X_train.shape)\n",
        "print(\"The shape of test input is:\",X_test.shape)\n",
        "\n",
        "print(\"The shape of total output is:\",Y.shape)\n",
        "print(\"The shape of training output is:\",Y_train.shape)\n",
        "print(\"The shape of test output is:\",Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ntU4ger8wA"
      },
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXs3btQvjH_"
      },
      "source": [
        "Let's fit our first machine learning model! We are going to first fit a linear regression, which is the simplest model we will consider since it has no hyperparameters.\n",
        "\n",
        "At a minimum this process takes two steps.\n",
        "\n",
        "First we create the model object, specifying any hyperparameters (but there are none for now). In this case we want to use the `LinearRegression` object from `sklearn.linear_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0ArrMz1IthGn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor_LinReg = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0zkZimvm2Q"
      },
      "source": [
        "Second we have to actually fit the linear regression model using our training data. This is done by calling the `fit()` method on our model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYPHVBJvsnB",
        "outputId": "483a9923-1ea1-4471-c18e-52ef9061d8be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regressor_LinReg.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQlOWLImv3tr"
      },
      "source": [
        "We can now make predictions for our test dataset using our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJt3Ymrsv6iI",
        "outputId": "fead04c3-0c41-46e2-9ab2-c5bd51907eda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([241.84730258, 250.12303941, 164.96456549, 119.11639346,\n",
              "       188.23120303, 260.56079379, 113.07583812, 190.54117538,\n",
              "       151.8883747 , 236.50848375, 168.76844138, 180.52719713,\n",
              "       109.16037049,  90.20148392, 244.73990469,  90.58113696,\n",
              "       152.51268196,  66.97735025,  98.0467335 , 215.39557064,\n",
              "       197.70737206, 160.9176914 , 162.88584001, 158.25373793,\n",
              "       202.44823294, 168.46663088, 119.87243699,  83.05669211,\n",
              "       189.9839726 , 163.02279586, 177.07828326,  82.6702699 ,\n",
              "       144.53204953, 146.07901596, 141.73841253, 195.18658206,\n",
              "       164.18043648, 189.14768927, 128.13330927, 206.12996392,\n",
              "        82.64273523, 164.94912645, 144.46057692, 182.0519825 ,\n",
              "       178.41355601,  72.5504089 , 142.69750371, 140.43671531,\n",
              "       121.75256103, 233.70553551, 162.07809758,  76.90270416,\n",
              "       155.68916375, 156.64052259, 238.11357481, 175.75735587,\n",
              "       190.82555855, 119.48230582, 131.3142863 , 172.2453037 ,\n",
              "       214.44479397, 171.30900357, 156.69146772, 110.9755974 ,\n",
              "       257.79427463, 154.6473623 ,  81.10560078, 227.26610365,\n",
              "       205.445138  ,  46.92383044,  78.28098211, 131.5335209 ,\n",
              "       105.63850688, 145.15896592, 133.85511669, 189.31799802,\n",
              "        99.29573544, 202.16752252, 221.67724342, 189.36681415,\n",
              "       150.25345002, 208.85311584,  48.03404661, 206.61925845,\n",
              "        76.63162703,  92.70630395, 148.19321808, 194.60105182,\n",
              "       133.07986975, 148.57002105,  97.51272455, 124.56889921,\n",
              "        82.48851826, 151.41937342, 124.81463753, 105.29002279,\n",
              "       236.2130895 , 227.49222218, 128.37518135, 164.29601063,\n",
              "       193.35752892, 111.83831907, 204.3691    ,  84.7480368 ,\n",
              "       217.70082325, 113.6130274 , 221.22851097, 267.69707418,\n",
              "       115.62235754, 113.98308423, 195.01301149])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LinReg_pred = regressor_LinReg.predict(X_test)\n",
        "LinReg_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "artWZM1Q_Wqz"
      },
      "source": [
        "We can then use the `score()` method to get a quick idea of how our model did on our training and testing data. For regression, the evaluation metric used for score is $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiMcKvQ2_Wq0",
        "outputId": "01ea13e1-2419-4500-cf17-c95b175f0926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training R-squared:\n",
            "0.555437148935302\n",
            "Testing R-squared:\n",
            "0.35940090989715556\n"
          ]
        }
      ],
      "source": [
        "print(\"Training R-squared:\")\n",
        "print(regressor_LinReg.score(X_train, Y_train))\n",
        "print(\"Testing R-squared:\")\n",
        "print(regressor_LinReg.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_xHJYlKcK-"
      },
      "source": [
        "#### Artificial Neural Network for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhMzhu83KcK_"
      },
      "source": [
        "Now we are going to fit a much more complicated model to our data: an artificial neural network (ANN). ANNs can be very accurate; however, they have lots of hyperparameters that are hard to get right.\n",
        "\n",
        "In `scikit-learn` the type of neural network we are using is an MLP, which stands for \"multi-layer perceptron.\" \n",
        "\n",
        "We start by seeing what possible hyperparameters our ANN has and what their default values are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Les6YL4T_Wq1",
        "outputId": "e921ac5a-3d5d-48fe-c10d-2237b9a313c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.0001,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (100,),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.001,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 200,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': None,\n",
              " 'shuffle': True,\n",
              " 'solver': 'adam',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': False,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "MLPRegressor().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXtJC-Gf_Wq2"
      },
      "source": [
        "Now we can set up an ANN model, this time specifying specific values for hyperparameters like the number of hidden layers. All hyperparameters that we don't specify values for will have their default values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "u1IhG5bQKcK_"
      },
      "outputs": [],
      "source": [
        "regressor_ANN_default = MLPRegressor(solver='lbfgs', max_iter=5000, hidden_layer_sizes=(40,1), learning_rate_init=0.000001, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kkFO9OI_Wq2"
      },
      "source": [
        "Next we fit this specific model with the `fit()` method and evaluate it with `score()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKZVws6t_Wq3",
        "outputId": "2f9f8cb8-fe9f-4257-d94a-05ea11f62908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training R-squared: \n",
            "0.6193483550783844\n",
            "Testing R-squared: \n",
            "0.34888647829370967\n"
          ]
        }
      ],
      "source": [
        "regressor_ANN_default.fit(X_train, Y_train)\n",
        "print(\"Training R-squared: \")\n",
        "print(regressor_ANN_default.score(X_train, Y_train))\n",
        "print(\"Testing R-squared: \")\n",
        "print(regressor_ANN_default.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r27sd4IPZsW"
      },
      "source": [
        "So we just fit an ANN with one set of hyperparameters.\n",
        "\n",
        "But our choice of hyperparameters is going to have a big effect on how well our model does. Thus, ideally we want to try multiple combinations of hyperparameters and then pick the best of these combinations.\n",
        "\n",
        "The first method we use for this process of choosing hyperparameters, called \"hyperparameter tuning\", is the `GridSearchCV()` function. We give it specific values of hyperparameters we want to be considered. These hyperparameters are detailed using a dictionary of lists, where the key is the hyperparameter name and the list contains values we want considered for that hyperparameter.\n",
        "\n",
        "Then when we fit our `GridSearchCV` object, we will actually be fitting many ANNs at once, one for each combination of hyperparameters. As part of the fitting process, Python does cross validation (CV) on each model and picks the best combination based on the model with the best overall evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R-sVlyILri3",
        "outputId": "4d44ef10-17f9-4a5e-d7a9-239c62d21467"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=MLPRegressor(max_iter=5000, solver='lbfgs'),\n",
              "             param_grid={'hidden_layer_sizes': [(5, 1), (40, 1), (50, 1)],\n",
              "                         'learning_rate_init': [0.0001, 1e-06]})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "ANN = MLPRegressor(solver='lbfgs', max_iter=5000)\n",
        "parameters = {'learning_rate_init':[0.0001, 0.000001], 'hidden_layer_sizes':[(5,1), (40,1), (50,1)]}\n",
        "regressor_ANN_tuned_grid = GridSearchCV(ANN, parameters)\n",
        "\n",
        "regressor_ANN_tuned_grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJoaF_SB_Wq8"
      },
      "source": [
        "To see the best combination of hyperparameters, we can access the `best_estimator_` field of our fitted object `regressor_ANN_tuned_grid`. And to see how well this best combination does, we can access the `best_score_` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcrkGy2h_Wq9",
        "outputId": "36a06829-bfd6-4b04-9190-bc8ad387adb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hidden_layer_sizes': (50, 1), 'learning_rate_init': 1e-06}\n",
            "Training R-squared:\n",
            "0.6661202079515617\n",
            "Testing R-squared:\n",
            "0.21072727904692645\n"
          ]
        }
      ],
      "source": [
        "print(regressor_ANN_tuned_grid.best_params_)\n",
        "print(\"Training R-squared:\")\n",
        "print(regressor_ANN_tuned_grid.score(X_train, Y_train))\n",
        "print(\"Testing R-squared:\")\n",
        "print(regressor_ANN_tuned_grid.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHSGQkqh_Wq-"
      },
      "source": [
        "Again we can what are model predicts using the `predict()` method on our model object. Although we trained multiple ANNs at once, it will by default give us predictions from the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKeB6KumRoQ0",
        "outputId": "35f51bb8-1c71-4526-c7ac-ce1f56619732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([262.10580023, 252.33818711, 135.74356361, 131.05355284,\n",
              "       139.52130287, 253.70620429,  93.40720843, 222.26355152,\n",
              "       162.10660515, 241.60687939, 101.31525165, 183.85997841,\n",
              "       113.29121402,  88.58004325, 265.7108572 ,  66.24802658,\n",
              "       137.20943263,  53.17456277, 171.29856011, 223.94281014,\n",
              "       151.72092861, 153.09812577, 148.05524613, 130.68066176,\n",
              "       213.29389033, 158.01520217, 148.31440019, 108.29318478,\n",
              "       156.13566128, 170.28897101, 159.8563813 ,  79.90823999,\n",
              "       126.57776315, 258.78749469, 127.25455847, 202.6296509 ,\n",
              "       192.4264755 , 114.64675685, 112.28510145, 210.40322735,\n",
              "        78.56607447, 150.00401197, 131.64085243, 194.65878282,\n",
              "       190.75048695,  88.1003597 , 128.35317635, 115.71076053,\n",
              "       124.67511276, 259.34271089, 132.82943688,  54.84324367,\n",
              "       110.54300696, 177.08980465, 256.71976786, 235.85507686,\n",
              "       195.03789232, 101.70219128, 114.98756432, 196.89646227,\n",
              "       218.3858139 , 167.36204638, 117.91568261, 120.17079961,\n",
              "       252.99008595, 165.1940764 ,  77.49091788, 243.3948655 ,\n",
              "       274.81893164,  16.30749871,  90.59865373, 153.6438978 ,\n",
              "        99.5817967 , 120.51274302,  96.57290213,  82.88547211,\n",
              "        90.68380373, 211.96283358, 230.62087889, 186.01969796,\n",
              "       146.76066029, 218.40393806,  49.40809246, 239.70123526,\n",
              "        73.37149312,  96.25415786, 150.52704851, 217.90883134,\n",
              "       156.37038117, 160.4485354 , 149.77047225, 101.20026531,\n",
              "        62.09698197, 163.65298565,  74.38996389,  96.59134512,\n",
              "       237.04967797, 209.05142632, 119.32524837, 160.26811013,\n",
              "       182.58548747, 139.84746468, 161.9816422 , 101.70522809,\n",
              "       240.60875998, 117.48309739, 193.44996448, 256.79973321,\n",
              "       146.49575157,  96.18199025, 214.4960141 ])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ANN_pred = regressor_ANN_tuned_grid.predict(X_test)\n",
        "ANN_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbtu3t5bwXWf"
      },
      "source": [
        "#### Performance Assessment using Testing Data\n",
        "\n",
        "We will compare the two models using these four metrics:\n",
        "*   Mean Absolute Error\n",
        "*   Mean Absolute Percentage Error\n",
        "*   Mean Square Error\n",
        "*   Root-Mean-Square Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEexDKqYx3cA"
      },
      "source": [
        "First, let's compute Mean Absolute Error (MAE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr21dcF-wqao",
        "outputId": "f5b0afc8-1917-4fe9-fdc1-93d0c6e83fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MAE of predictions provided by GLM is :\n",
            "45.120987683251\n",
            "The MAE of predictions provided by ANN is :\n",
            "49.8220110658498\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "MAE_GLM = mean_absolute_error(Y_test, LinReg_pred)\n",
        "MAE_ANN = mean_absolute_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MAE of predictions provided by GLM is :\")\n",
        "print(MAE_GLM)\n",
        "\n",
        "print(\"The MAE of predictions provided by ANN is :\")\n",
        "print(MAE_ANN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZCsUF1EyATg"
      },
      "source": [
        "Second, let's compute Mean Absolute Percentage Error (MAPE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpCMtL9S3U0h",
        "outputId": "633b3c3f-b6cd-4fc6-8ae4-df0df10dd24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MAPE of predictions provided by GLM is :\n",
            "0.37961401187552524\n",
            "The MAPE of predictions provided by ANN is :\n",
            "0.40539515712576035\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "MAPE_GLM = mean_absolute_percentage_error(Y_test, LinReg_pred)\n",
        "MAPE_ANN = mean_absolute_percentage_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MAPE of predictions provided by GLM is :\")\n",
        "print(MAPE_GLM)\n",
        "\n",
        "print(\"The MAPE of predictions provided by ANN is :\")\n",
        "print(MAPE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2neFZEkyIJh"
      },
      "source": [
        "Third, let's compute Mean Square Error (MSE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v44igycf39JO",
        "outputId": "8275dc82-27be-4b55-bc92-8068d15726c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MSE of predictions provided by GLM is :\n",
            "3180.1988368427265\n",
            "The MSE of predictions provided by ANN is :\n",
            "3918.276232212077\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_GLM = mean_squared_error(Y_test, LinReg_pred)\n",
        "MSE_ANN = mean_squared_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MSE of predictions provided by GLM is :\")\n",
        "print(MSE_GLM)\n",
        "\n",
        "print(\"The MSE of predictions provided by ANN is :\")\n",
        "print(MSE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKo8f8k5yOEh"
      },
      "source": [
        "Last, let's compute Root-Mean-Suqare-Error (RMSE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aBlPKXL5oAh",
        "outputId": "190e79e9-f55c-45d0-9d8f-afd5f0cd9171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The RMSE of predictions provided by GLM is :\n",
            "56.39325169594964\n",
            "The RMSE of predictions provided by ANN is :\n",
            "62.59613592077451\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "RMSE_GLM = mean_squared_error(Y_test, LinReg_pred, squared=False)\n",
        "RMSE_ANN = mean_squared_error(Y_test, ANN_pred, squared=False)\n",
        "\n",
        "print(\"The RMSE of predictions provided by GLM is :\")\n",
        "print(RMSE_GLM)\n",
        "\n",
        "print(\"The RMSE of predictions provided by ANN is :\")\n",
        "print(RMSE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SarBlYm2WQ4x"
      },
      "source": [
        "## Supervised Learning - Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULypkTWU9hEp"
      },
      "source": [
        "#### Dataset Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29YqLUpTWQ4z"
      },
      "source": [
        "Now let's look at a classification problem, where the outcome can only take on 2 or more discrete values. But of course our predictors can be either continuous or discrete.\n",
        "\n",
        "Now we use `scikit-learn`'s breast cancer dataset. Here the outcome can take on a 0, for no breast cancer, or 1, for breast cancer. \n",
        "\n",
        "So in this case we actually have a *binary classification* problem, meaning our category can only take on 2 discrete values. In most binary classification problems, like in this case, the categories are 0 and 1 indiciating the presence or absence of some trait."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "0Jmrv4VMWQ41"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZau-rgMWQ41",
        "outputId": "9bdfadbc-4068-4302-f45a-b8635c2671a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    357\n",
              "0    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer.target.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Kl5F0QBtSa"
      },
      "source": [
        "The `breast cancer` dataset contains 30 predictive variables. For example:\n",
        "\n",
        "\n",
        "\n",
        "*    radius (mean of distances from center to points on the perimeter)\n",
        "\n",
        "*    texture (standard deviation of gray-scale values)\n",
        "\n",
        "*    perimeter\n",
        "\n",
        "*    area\n",
        "\n",
        "*    smoothness (local variation in radius lengths)\n",
        "\n",
        "*    compactness (perimeter^2 / area - 1.0)\n",
        "\n",
        "*    concavity (severity of concave portions of the contour)\n",
        "\n",
        "*    concave points (number of concave portions of the contour)\n",
        "\n",
        "*    symmetry\n",
        "\n",
        "*    fractal dimension (“coastline approximation” - 1)\n",
        "\n",
        "\n",
        "\n",
        "The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "NU-R05ZjXEbr",
        "outputId": "0f2977d9-51a3-4893-9fa2-f2d6b8884e45"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "0                   0.07871  ...        25.380          17.33   \n",
              "1                   0.05667  ...        24.990          23.41   \n",
              "2                   0.05999  ...        23.570          25.53   \n",
              "3                   0.09744  ...        14.910          26.50   \n",
              "4                   0.05883  ...        22.540          16.67   \n",
              "..                      ...  ...           ...            ...   \n",
              "564                 0.05623  ...        25.450          26.40   \n",
              "565                 0.05533  ...        23.690          38.25   \n",
              "566                 0.05648  ...        18.980          34.12   \n",
              "567                 0.07016  ...        25.740          39.42   \n",
              "568                 0.05884  ...         9.456          30.37   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "0             184.60      2019.0           0.16220            0.66560   \n",
              "1             158.80      1956.0           0.12380            0.18660   \n",
              "2             152.50      1709.0           0.14440            0.42450   \n",
              "3              98.87       567.7           0.20980            0.86630   \n",
              "4             152.20      1575.0           0.13740            0.20500   \n",
              "..               ...         ...               ...                ...   \n",
              "564           166.10      2027.0           0.14100            0.21130   \n",
              "565           155.00      1731.0           0.11660            0.19220   \n",
              "566           126.70      1124.0           0.11390            0.30940   \n",
              "567           184.60      1821.0           0.16500            0.86810   \n",
              "568            59.16       268.6           0.08996            0.06444   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "0             0.7119                0.2654          0.4601   \n",
              "1             0.2416                0.1860          0.2750   \n",
              "2             0.4504                0.2430          0.3613   \n",
              "3             0.6869                0.2575          0.6638   \n",
              "4             0.4000                0.1625          0.2364   \n",
              "..               ...                   ...             ...   \n",
              "564           0.4107                0.2216          0.2060   \n",
              "565           0.3215                0.1628          0.2572   \n",
              "566           0.3403                0.1418          0.2218   \n",
              "567           0.9387                0.2650          0.4087   \n",
              "568           0.0000                0.0000          0.2871   \n",
              "\n",
              "     worst fractal dimension  \n",
              "0                    0.11890  \n",
              "1                    0.08902  \n",
              "2                    0.08758  \n",
              "3                    0.17300  \n",
              "4                    0.07678  \n",
              "..                       ...  \n",
              "564                  0.07115  \n",
              "565                  0.06637  \n",
              "566                  0.07820  \n",
              "567                  0.12400  \n",
              "568                  0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnasUbL_9xOg"
      },
      "source": [
        "#### Define the predictor variable set and a target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQALRlB997O7"
      },
      "source": [
        "For the regression tasks, a feature set `X` and a target set `y` need to be defined first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "eWj12b_r98X8",
        "outputId": "e2c56145-fb79-43b2-d5e0-f45caeba7bea"
      },
      "outputs": [],
      "source": [
        "X = breast_cancer.data\n",
        "Y = breast_cancer.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eOA-Y_9lcz"
      },
      "source": [
        "#### Split the dataset into training and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89bteG4dHsXY"
      },
      "source": [
        "Again we use `sklearn.model_selection.train_test_split()` to do the random split for the training and testing dataset.\n",
        "\n",
        "Note: Here, `stratify` assures that the testing dataset has equal numbers of 0's and 1's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "HU4MmjqLHtaQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, stratify=Y,  random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elJ87AHnKp0t"
      },
      "source": [
        "#### XGBoost for Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoavtYx4_WrN"
      },
      "source": [
        "The first classification model we will try is XGBoost, considered one of the best models out there.\n",
        "\n",
        "Again we start by seeing what hyperparameters we can possibly tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yvLrjCf_WrN",
        "outputId": "0c7a6b01-ef06-40c7-bcb4-9db4bcbabd07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.0,\n",
              " 'criterion': 'friedman_mse',\n",
              " 'init': None,\n",
              " 'learning_rate': 0.1,\n",
              " 'loss': 'deviance',\n",
              " 'max_depth': 3,\n",
              " 'max_features': None,\n",
              " 'max_leaf_nodes': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_iter_no_change': None,\n",
              " 'random_state': None,\n",
              " 'subsample': 1.0,\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GradientBoostingClassifier().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX6uFoxntutW"
      },
      "source": [
        "Now let's create a model with specific hyperparameters. We will name it as `classifier_XGB`.\n",
        "\n",
        "In this case for classification, the score we get is the overall accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apugvOWmtt8G",
        "outputId": "09c94242-e227-4dec-f6d8-9fedce599f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for training dataset:\n",
            "0.9949748743718593\n",
            "Accuracy for testing dataset:\n",
            "0.9590643274853801\n"
          ]
        }
      ],
      "source": [
        "classifier_XGB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0)\n",
        "classifier_XGB.fit(X_train, Y_train)\n",
        "print(\"Accuracy for training dataset:\")\n",
        "print(classifier_XGB.score(X_train, Y_train))\n",
        "print(\"Accuracy for testing dataset:\")\n",
        "print(classifier_XGB.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvJDUYxk_WrO"
      },
      "source": [
        "But again we probably want to do some hyperparameter tuning.\n",
        "\n",
        "The second main way to do this is `RandomizedSearchCV()`. Here we give distributions for our hyperparameters instead of specific values. Python will then randomly choose hyperparameters to try based on the given distributions.\n",
        "\n",
        "For example, for XGBoost we will use a normal distribution with mean of 0.5 and standard deviation of 0.1 for the \"minimum impurity decrease\". This means we will mostly try values close to 0.5, but occasionally some further from 0.5. We will then consider a uniform distribution for learning rate between 0 and 1, meaning any number in this range is equally likely to be chosen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "A0jgh8n-_WrO"
      },
      "outputs": [],
      "source": [
        "XGB = GradientBoostingClassifier(n_estimators=100)\n",
        "\n",
        "from scipy.stats import norm, uniform\n",
        "distributions = {\"min_impurity_decrease\":norm(loc=0.5, scale=0.1), \"learning_rate\":uniform(loc=0.5, scale=0.5)}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "classifier_XGB_tuned_random = RandomizedSearchCV(XGB, distributions, n_iter=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ga87hbM_WrQ"
      },
      "source": [
        "Now we fit our `RandomizedSearchCV` object. Since `n_iter` is 10 above, we will grab 10 combinations of hyperparameters from our two distributions. Then we will fit an XGBoost model for each combination, Python choosing the best one for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOBVHyUC_WrQ",
        "outputId": "c6392c04-ab40-4324-c703-ebcf4ada5da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.6462933398888173, 'min_impurity_decrease': 0.5342229738705377}\n",
            "Accuracy for training dataset:\n",
            "0.9849246231155779\n",
            "Accuracy for testing dataset:\n",
            "0.9239766081871345\n"
          ]
        }
      ],
      "source": [
        "classifier_XGB_tuned_random.fit(X_train, Y_train)\n",
        "XGB_pred = classifier_XGB_tuned_random.predict(X_test)\n",
        "\n",
        "print(classifier_XGB_tuned_random.best_params_)\n",
        "print(\"Accuracy for training dataset:\")\n",
        "print(classifier_XGB_tuned_random.score(X_train, Y_train))\n",
        "print(\"Accuracy for testing dataset:\")\n",
        "print(classifier_XGB_tuned_random.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIQhfBLiK0Zn"
      },
      "source": [
        "#### Support Vector Machine for Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp-UWCS8uBY0"
      },
      "source": [
        "Lastly, we will create an SVM model. \n",
        "\n",
        "We will just go back to using `GridSearchCV()` for our hyperparameter tuning. To know what to tune, let's see what the hyperparameters are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-RYcxNYuQRJ",
        "outputId": "6468a485-c6d7-4453-b7f8-931cf04c2c9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'C': 1.0,\n",
              " 'break_ties': False,\n",
              " 'cache_size': 200,\n",
              " 'class_weight': None,\n",
              " 'coef0': 0.0,\n",
              " 'decision_function_shape': 'ovr',\n",
              " 'degree': 3,\n",
              " 'gamma': 'scale',\n",
              " 'kernel': 'rbf',\n",
              " 'max_iter': -1,\n",
              " 'probability': False,\n",
              " 'random_state': None,\n",
              " 'shrinking': True,\n",
              " 'tol': 0.001,\n",
              " 'verbose': False}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "SVC().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enxTf2PI_WrS"
      },
      "source": [
        "We will now create and fit our model, leaving `gamma` fixed at `auto` and tuning `C`, which is by far the most important hyperparameter to get right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKgykWtx_WrT",
        "outputId": "1d88d7c5-21dc-4cb4-9864-a7408c618de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.001}\n",
            "0.6281645569620252\n"
          ]
        }
      ],
      "source": [
        "SVC_model = SVC(gamma='auto', probability=True)\n",
        "\n",
        "parameters = {\"C\":[0.001, 0.1, 0.5, 1, 1.5]}\n",
        "\n",
        "classifier_SVM_tuned_grid = GridSearchCV(SVC_model, parameters)\n",
        "\n",
        "classifier_SVM_tuned_grid.fit(X_train, Y_train)\n",
        "print(classifier_SVM_tuned_grid.best_params_)\n",
        "print(classifier_SVM_tuned_grid.best_score_)\n",
        "\n",
        "SVM_pred = classifier_SVM_tuned_grid.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1fxh88pvQzj"
      },
      "source": [
        "#### Performance Assessment using Testing Data\n",
        "\n",
        "*   Confusion Matrix\n",
        "*   Accuracy\n",
        "*   Sensitivity\n",
        "*   Specificity\n",
        "*   AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDaIRwwW4BZi"
      },
      "source": [
        "We start by assessing XGBoost:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "sqsmOh8fz9Gn",
        "outputId": "e7f0a179-90e6-4843-c06c-898aeb2c7bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix for XGB: \n",
            " [[ 56   8]\n",
            " [  5 102]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#Compute the confusion matrix\n",
        "cmXGB = confusion_matrix(Y_test,XGB_pred)\n",
        "print('Confusion Matrix for XGB: \\n', cmXGB)\n",
        "\n",
        "#Plot the confusion matrix using seaborn\n",
        "import seaborn as sns\n",
        "sns.heatmap(cmXGB, annot=True,fmt=\"d\", cmap=\"YlGnBu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ7FFlnwElfz",
        "outputId": "12b1f99b-89ca-4bb4-90d9-4de94eeae55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for XGB:  0.9239766081871345\n",
            "Specificity for XGB:  0.9532710280373832\n",
            "Sensitivity for XGB:  0.875\n"
          ]
        }
      ],
      "source": [
        "#Compute total test cases\n",
        "totalXGB=sum(sum(cmXGB))\n",
        "#####from confusion matrix calculate accuracy\n",
        "accuracyXGB=(cmXGB[0,0]+cmXGB[1,1])/totalXGB\n",
        "print ('Accuracy for XGB: ', accuracyXGB)\n",
        "\n",
        "sensitivityXGB = cmXGB[1,1]/(cmXGB[1,0]+cmXGB[1,1])\n",
        "print('Specificity for XGB: ', sensitivityXGB)\n",
        "\n",
        "specificityXGB = cmXGB[0,0]/(cmXGB[0,0]+cmXGB[0,1])\n",
        "print('Sensitivity for XGB: ', specificityXGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ekaseon1kDu"
      },
      "source": [
        "Then, we compute the AUC. We use the `roc_auc_score` from `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8eqsAS210uf",
        "outputId": "ca8b341e-4e8c-4961-de5c-a452f4facc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC for XGB:  0.9763434579439252\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#We first compute the probability output from the SVM classifier\n",
        "XGB_prob = classifier_XGB_tuned_random.predict_proba(X_test)\n",
        "\n",
        "#We compute the AUC score\n",
        "aucSVM = roc_auc_score(Y_test,XGB_prob[:,1])\n",
        "print('AUC for XGB: ', aucSVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNTgE26_WrW"
      },
      "source": [
        "Now for SVM's prediction, we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "E9gsOWuR_WrW",
        "outputId": "0cf5ec7d-b3c7-4411-9d89-8f2d561fe454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix for SVM: \n",
            " [[  0  64]\n",
            " [  0 107]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#Compute the confusion matrix\n",
        "cmSVM = confusion_matrix(Y_test,SVM_pred)\n",
        "print('Confusion Matrix for SVM: \\n', cmSVM)\n",
        "\n",
        "#Plot the confusion matrix using seaborn\n",
        "import seaborn as sns\n",
        "sns.heatmap(cmSVM, annot=True,fmt=\"d\", cmap=\"YlGnBu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDXSmoA0E6QI",
        "outputId": "4e4fbf8a-ac64-427f-8d55-207f6940b2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for SVM:  0.6257309941520468\n",
            "Specificity for SVM:  1.0\n",
            "Sensitivity for SVM:  0.0\n"
          ]
        }
      ],
      "source": [
        "#Compute total test cases\n",
        "totalSVM=sum(sum(cmSVM))\n",
        "#####from confusion matrix calculate accuracy\n",
        "accuracySVM=(cmSVM[0,0]+cmSVM[1,1])/totalSVM\n",
        "print ('Accuracy for SVM: ', accuracySVM)\n",
        "\n",
        "sensitivitySVM = cmSVM[1,1]/(cmSVM[1,0]+cmSVM[1,1])\n",
        "print('Specificity for SVM: ', sensitivitySVM)\n",
        "\n",
        "specificitySVM = cmSVM[0,0]/(cmSVM[0,0]+cmSVM[0,1])\n",
        "print('Sensitivity for SVM: ', specificitySVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzeHoOb_WrW"
      },
      "source": [
        "Then, we compute the AUC for SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6N8mE2x_WrW",
        "outputId": "0d688f11-da66-4aee-d599-4ae38489f922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC for SVM:  0.8027161214953271\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#We first compute the probability output from the SVM classifier\n",
        "SVM_prob = classifier_SVM_tuned_grid.predict_proba(X_test)\n",
        "\n",
        "#We compute the AUC score\n",
        "aucSVM = roc_auc_score(Y_test,SVM_prob[:,1])\n",
        "print('AUC for SVM: ', aucSVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX7SnsiDWQ43"
      },
      "source": [
        "## Unsupervised Learning - Clustering (and Dimension Reduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORhQRZbv7Cwc"
      },
      "source": [
        "#### Dataset Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5MeAadHWQ44"
      },
      "source": [
        "Lastly we want to consider an unsupervised learning problem, where we don't actually have an outcome at all, or our data is \"unlabeled.\" Instead of predicting something we just want to find patterns and structure in our data.\n",
        "\n",
        "Our data can be unlabeled for two reasons:\n",
        "\n",
        "• First, maybe our data does not have well-defined groupings. An example might be a company's customers: there are not clear and distinct groups that we can put people in. \n",
        "\n",
        "• Second, maybe the label of our data is missing. Suppose you are a wine vendor and you ordered three types of wine from your supplier. When you receive your wine shipment, however, you realize that the labels were not put on. You may want to learn about how the different wine bottles are related to one another to make an educated guess on which is what wine type.\n",
        "\n",
        "Our example below falls in this second case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "SL-PO7LrWQ45"
      },
      "outputs": [],
      "source": [
        "wine = datasets.load_wine(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4l4wK5z5YUC6",
        "outputId": "57b2de59-c93b-4415-aab5-d200c7fde0ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "..       ...         ...   ...                ...        ...            ...   \n",
              "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
              "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
              "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
              "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
              "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
              "\n",
              "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0          3.06                  0.28             2.29             5.64  1.04   \n",
              "1          2.76                  0.26             1.28             4.38  1.05   \n",
              "2          3.24                  0.30             2.81             5.68  1.03   \n",
              "3          3.49                  0.24             2.18             7.80  0.86   \n",
              "4          2.69                  0.39             1.82             4.32  1.04   \n",
              "..          ...                   ...              ...              ...   ...   \n",
              "173        0.61                  0.52             1.06             7.70  0.64   \n",
              "174        0.75                  0.43             1.41             7.30  0.70   \n",
              "175        0.69                  0.43             1.35            10.20  0.59   \n",
              "176        0.68                  0.53             1.46             9.30  0.60   \n",
              "177        0.76                  0.56             1.35             9.20  0.61   \n",
              "\n",
              "     od280/od315_of_diluted_wines  proline  \n",
              "0                            3.92   1065.0  \n",
              "1                            3.40   1050.0  \n",
              "2                            3.17   1185.0  \n",
              "3                            3.45   1480.0  \n",
              "4                            2.93    735.0  \n",
              "..                            ...      ...  \n",
              "173                          1.74    740.0  \n",
              "174                          1.56    750.0  \n",
              "175                          1.56    835.0  \n",
              "176                          1.62    840.0  \n",
              "177                          1.60    560.0  \n",
              "\n",
              "[178 rows x 13 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wine.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5r4Un57I2-"
      },
      "source": [
        "#### Initialize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzcQqq5r-QNs"
      },
      "source": [
        "Initialize the dataset as `X`. Although we have actual labels for the wines in this dataset, splitting them into three types, we are going to ignore them for now. This is because we are trying to mimic unsupervised learning where we do not know the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ffGyVj1w-S7v"
      },
      "outputs": [],
      "source": [
        "X= wine.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0yRsiFz-_K0"
      },
      "source": [
        "#### Clustering using K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gyXR2eUBcfO"
      },
      "source": [
        "##### Intialize the K-Means Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u22oqn0-IRO"
      },
      "source": [
        "The first of two algorithms we will use for clustering is k-means.\n",
        "\n",
        "We can create this model by defining the number of clusters with `n_clusters` and a random initialization state so that we all get the same results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN01QBb4-Pdt",
        "outputId": "b92f0661-9c50-49d8-e4a9-40416f55731d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=3, random_state=0)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "#Here we set n=3 in this workshop\n",
        "cluster_KMeans = KMeans(n_clusters=3, random_state=0)\n",
        "cluster_KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGumzwgl-1xt"
      },
      "source": [
        "##### Fit and predict using K-Mean algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_ch__Ce-1Gx",
        "outputId": "40622868-8c92-45fc-d6b6-f03adbe4f57a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=3, random_state=0)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_KMeans.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBqEwxW_hoE",
        "outputId": "7aa8bcbd-5393-4a14-9b0b-f2567b7e0575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 2, 0,\n",
              "       2, 2, 0, 0, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0,\n",
              "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
              "       2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 0,\n",
              "       0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0,\n",
              "       0, 2], dtype=int32)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_labels = cluster_KMeans.predict(X)\n",
        "cluster_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEL2KmZH1e3y"
      },
      "source": [
        "Compute inner cluster distances (Euclidean distances) within all clusters. This gives us a sense of how good our clustering is. The smaller the number, the better. But the raw number below is hard to assess since we have nothing to compare it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLcypDGJ1kSZ",
        "outputId": "51a00f7c-7a91-4498-ff12-dd0f9db76b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2370689.686782969"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_KMeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2VQS91NBUJN"
      },
      "source": [
        "##### Find the BEST k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcA2Ed-z1toI"
      },
      "source": [
        "Above we immediately tried to make 3 clusters since we know that in reality, there are 3 wine types. But suppose we did not know how many there were.\n",
        "\n",
        "Then we would try to cluster our data into multiple numbers of groups and see which is best. This is what we do below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "yVW1JGmP1s0P"
      },
      "outputs": [],
      "source": [
        "#Store the sum of inner cluster distances in a list and name is as dist\n",
        "dist=[]\n",
        "\n",
        "for i in range(2,11):\n",
        "  #Run k-mean clustering with i centers\n",
        "  cluster_KMeans = KMeans(n_clusters =i, random_state=0)\n",
        "  cluster_KMeans.fit(X)\n",
        "  #Store the summed inner cluster distances into the list namely 'dist'\n",
        "  dist.append(cluster_KMeans.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aRiilxo8bGJ"
      },
      "source": [
        "Now dist tells us how well each number of groupings does in putting data points together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPTDSExDBY0v",
        "outputId": "5c5585a9-bcbd-4fa1-8163-e5b5814c032d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4543749.614531862,\n",
              " 2370689.686782968,\n",
              " 1331903.0622637183,\n",
              " 916379.187153917,\n",
              " 647326.0020260847,\n",
              " 412137.50910045847,\n",
              " 324523.6250001953,\n",
              " 270954.92924153747,\n",
              " 217887.378560333]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNud2CgJ8hYy"
      },
      "source": [
        "Now, let's use `matplotlib` library to visualize the inner cluster distances for the various values of k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dyljMec38NjX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MexfgXCq8o1Y"
      },
      "source": [
        "We can plot the inner cluster distance curve and use Elbow's method to seek for optimal k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "a8w02SxZ8P4j",
        "outputId": "813b19a6-0818-46b7-edb3-810c09de8748"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcbElEQVR4nO3deXDc5Z3n8fe3D52W5EOnJWFj4wPfRgoBEx8YiA1hgYRYIbOB7FY2QIVNQjazqcnM1GTZzFRtdiaZTMJuajxJJgdH1uaGBAcCxiQQDJIPLNv4vmRbl22d1q1n/+i2ET4lW63fr7s/ryqVW92t1qcE+ujp5/n9np855xAREf8KeB1AREQuTEUtIuJzKmoREZ9TUYuI+JyKWkTE51TUIiI+F7OiNrOfm1m9mVUP8vkVZrbNzLaa2ROxyiUiEm8sVsdRm9kioA34lXNu1kWeOwVYBSx1zp0ws3znXH1MgomIxJmYjaidc28CxwfeZ2aTzWyNmVWZ2R/NbHr0oS8D/8c5dyL6tSppEZGokZ6jXgl81TlXBvwl8H+j908FpprZW2b2jpktH+FcIiK+FRqpb2Rmo4AFwGozO3V36oAcU4AlQAnwppnNds41jVQ+ERG/GrGiJjJ6b3LOzTvHYzXAeudcD7DPzHYSKe73RjCfiIgvjdjUh3OuhUgJrwCwiLnRh58jMprGzHKJTIXsHalsIiJ+FsvD854E/gxMM7MaM/sS8B+BL5nZZmArcGf06b8HjpnZNmAt8N+dc8dilU1EJJ7E7PA8EREZHjozUUTE52KymJibm+smTpwYi5cWEUlIVVVVjc65vHM9FpOinjhxIpWVlbF4aRGRhGRmB873mKY+RER8TkUtIuJzKmoREZ9TUYuI+JyKWkTE51TUIiI+p6IWEfE53xR1Z08f//bmXt7e3eh1FBERX/FNUYcCxso/7uXnb+3zOoqIiK/4p6iDAe6+poS1Oxqob+n0Oo6IiG/4pqgBVpSX0NfveGbjYa+jiIj4hq+KenLeKMonjGFV5SG0/aqISISvihqgoryUvQ3tbDh4wusoIiK+4Luivm1OERkpQVa9V+N1FBERX/BdUY9KDfGp2UW89P4R2rt6vY4jIuI53xU1QMXHSmnv7uN3W456HUVExHO+LOryCWOYlJvJ6kpNf4iI+LKozYwV5aW8u/84exvavI4jIuIpXxY1wN3XFBMMGKurNKoWkeTm26LOz05jydQ8nq6qobev3+s4IiKe8W1RA6woL6W+tYs3dzV4HUVExDO+Luql0/MZl5miY6pFJKn5uqhTQgE+Pb+YP2yv41hbl9dxREQ84euihsj0R2+/41lt1CQiScr3RT2tMIu5paO1UZOIJC3fFzVARXkJO+vaeL+m2esoIiIjLi6K+j/MHU9qKMCqykNeRxERGXFxUdTZaWFum13EC5uO0NHd53UcEZERFRdFDZGrv7R29fL7rbVeRxERGVFxU9TXXTmOK8ZmaPpDRJLOoIvazIJmttHMXoploPMJBIwVZSW8vecYB4+d9CKCiIgnhjKi/jqwPVZBBuPushLM4KkqjapFJHkMqqjNrAT4FPDT2Ma5sPGj01k4JY+nqmro69cx1SKSHAY7ov4h8C3gvNvYmdn9ZlZpZpUNDbHbRKmivIQjzZ28tbsxZt9DRMRPLlrUZnY7UO+cq7rQ85xzK51z5c658ry8vGELeKZbZhQwOiOsRUURSRqDGVHfANxhZvuB3wBLzeyxmKa6gNRQkLvmFfPK1jqaTnZ7FUNEZMRctKidc992zpU45yYC9wCvO+e+EPNkF7CivITuvn6e33TEyxgiIiMibo6jHmjm+Bxmjs/W9IeIJIUhFbVz7g3n3O2xCjMUFeWlbD3SQvVhbdQkIoktLkfUAHfOG09KMMBTuvitiCS4uC3q0RkpfHJmAc9uPExnjzZqEpHEFbdFDfC5j5XS3NHDH7bXeR1FRCRm4rqoF0zOpXh0OqsqNf0hIokrros6GDDuLivhj7saONzU4XUcEZGYiOuiBlhRVoJz8LQWFUUkQcV9UZeOzWDB5HGsrjpEvzZqEpEEFPdFDZFjqg8d7+Cdfce8jiIiMuwSoqiXzyokKy3Eai0qikgCSoiiTgsHuWPueH635SgtnT1exxERGVYJUdQQmf7o6u3nxc3aqElEEkvCFPWckhymFWTpmGoRSTgJU9RmxoryEjYfamJHbavXcUREhk3CFDXAp+cXEwoYq7X9qYgkkIQq6nGjUrn56shGTd295728o4hIXEmooobIRk3H2rt5/YN6r6OIiAyLhCvqhVNyKchO1fSHiCSMhCvqUDDA3deUsHZHPXUtnV7HERG5bAlX1AArykvpd/D0Bh2qJyLxLyGL+srcTK6dOJbVlTU4p42aRCS+JWRRA6woL2FfYzuVB054HUVE5LIkbFHfNruIzJQgq97ToqKIxLeELerM1BC3zxnPb7ccpa2r1+s4IiKXLGGLGqDiYyWc7O7jd+8f9TqKiMglS+iivuaKMUzKy2SVjqkWkTiW0EVtZlSUl1J54AR7Gtq8jiMickkSuqgBPnNNMcGA6eovIhK3Er6o87PSuHFaPk9vqKG3Txs1iUj8SfiiBqgoL6GhtYt1Oxu8jiIiMmRJUdQ3Ts8nd1SKFhVFJC4lRVGHgwE+c00Jr22vp6G1y+s4IiJDkhRFDbCirITefsdzGw97HUVEZEiSpqinFGQx/4rRrKo8pI2aRCSuJE1RA1SUl7Krvo1Nh5q8jiIiMmhJVdS3zykiLRxglY6pFpE4klRFnZUW5rbZRby4+Qgd3X1exxERGZSkKmqITH+0dfXycrU2ahKR+HDRojazNDN718w2m9lWM3tkJILFysevHMuEcRk6plpE4sZgRtRdwFLn3FxgHrDczK6LaaoYMjNWlJXwzt7jHDjW7nUcEZGLumhRu4hTW8+Fox9xfXzb3WUlBAyeqtKiooj436DmqM0saGabgHrgVefc+nM8534zqzSzyoYGf++pUZSTzqKpeTxVVUNff1z/zRGRJDCoonbO9Tnn5gElwLVmNuscz1npnCt3zpXn5eUNc8zhV1FeytHmTv60u9HrKCIiFzSkoz6cc03AWmB5TNKMoJuuzmdMRliLiiLie4M56iPPzEZHb6cDtwAfxDhXzKWGgtw1v5hXt9Zxor3b6zgiIuc1mBF1EbDWzN4H3iMyR/1SbGONjBVlpXT39fPcJm3UJCL+FbrYE5xz7wPzRyDLiJsxPpvZxTn8v/cO8Z8WTMTMvI4kInKWpDsz8UwV5SV8UNvK1iMtXkcRETmnpC/qO+YWkxIKaFFRRHwr6Ys6JyPM8pmFPLfxMJ092qhJRPwn6YsaIsdUt3T28sq2Oq+jiIicRUUNLJg8juLR6azW9IeI+JCKGggEjM+WlfCn3Y3UnDjpdRwRkY9QUUetKC8B4OkqHVMtIv6ioo4qGZPBDZNzWV11iH5t1CQiPqKiHmBFeQk1Jzp4Z+8xr6OIiJymoh5g2cxCstNCOqZaRHxFRT1AWjjInfOKebm6luaOHq/jiIgAKuqzVJSX0tXbzwubj3gdRUQEUFGfZVZxNtMLs3RMtYj4hor6DGZGRXkp79c0s/2oNmoSEe+pqM/hrvnFhIPG6kpd/FZEvKeiPoexmSncMqOAZzfW0N3b73UcEUlyKurzWFFeyomTPby2XRs1iYi3VNTnsWhKHoXZaTqmWkQ8p6I+j2DAuLusmHU7G6ht7vQ6jogkMRX1BawoK6XfwdMbtKgoIt5RUV/AxNxMPn7lWFZXHsI5bdQkIt5QUV9ERXkp+4+d5L39J7yOIiJJSkV9EbfOLiQrNcQ/vbKDnj4dqiciI09FfREZKSH+510zeXffcf7ht9u9jiMiSSjkdYB48On5JWypaeHnb+1jdnEOd5eVeB1JRJKIRtSD9Ne3TWfB5HF8+9ktvF/T5HUcEUkiKupBCgUDPPoX15A3KpUHfl1FQ2uX15FEJEmoqIdgbGYK/3pvGSdOdvPQExu0uCgiI0JFPUSzinP43t1zeHffcf7+pW1exxGRJKDFxEtw57xittQ089M/7WNWcQ4ryku9jiQiCUwj6kv0V7dO54arxvE3z1Wz6VCT13FEJIGpqC9RKBjg0c9fQ35WKg/+uor6Vm3cJCKxoaK+DGMyU1h5bzlNHd089PgGXWRARGJCRX2ZZozP5n9/di7v7T/Bd7W4KCIxoMXEYXDH3PFUH25m5Zt7mV2cQ8XHtLgoIsNHI+ph8q1l01g4JZe/fa6ajQe1056IDJ+LFrWZlZrZWjPbZmZbzezrIxEs3oSCAX78+fkU5KTy4GNaXBSR4TOYEXUv8E3n3AzgOuAhM5sR21jxaXRGZHGxpaOXrzymxUURGR4XLWrn3FHn3Ibo7VZgO1Ac62Dx6uqibP5xxRwqD5zgkRe3eh1HRBLAkOaozWwiMB9Yf47H7jezSjOrbGhoGKZ48en2OeN5YPEkHl9/kN+8e9DrOCIS5wZd1GY2CngaeNg513Lm4865lc65cudceV5e3nBmjEvfWjadhVNy+bvnt7JBi4sichkGVdRmFiZS0o87556JbaTEEAwYP/78fApz0iJnLrZocVFELs1gjvow4GfAdufcD2IfKXGMzkhh5X1ltHX18uBjVXT19nkdSUTi0GBG1DcA9wJLzWxT9OO2GOdKGNMLs/nHz85lw8Em/scLOnNRRIbuomcmOuf+BNgIZElYn5pTRPWRyfzkjT3MLs7hLz5+hdeRRCSO6MzEEfKXn5zG4ql5fOeFaqoOHPc6jojEERX1CAkGjB/dM5/xo9N58LEN1GlxUUQGSUU9gnIywqy8t5x2LS6KyBCoqEfYtMIsvr9iLhsPNvGd57finPM6koj4nIraA7fOLuKhGyfzm/cO8fh6nbkoIhemovbIf7tlGkum5fHIi1up3K/FRRE5PxW1R4IB41/umU/JmAwefGwDtc1aXBSRc1NReygnPczKe8vo6O7lgceq6OzR4qKInE1F7bEpBVl8v2Iemw818XfPV2txUUTOoqL2geWzCvnq0qtYVVnDY+8c8DqOiPiMitonvnHzVJZOz+eRF7fx7j4tLorIh1TUPhEIGP/8uXmUjs3gK49XcbS5w+tIIuITKmofyUkP82/3ldHZ08+Dv9bioohEqKh95qr8LH5QMZfNNc387XNaXBQRFbUvfXJmIV+7aQpPVdXwqz9rcVEk2amoferhm6Zw89X5fPelbazfe8zrOCLiIRW1TwUCxg8+N48rxmXwlcc3cKRJi4siyUpF7WPZaZFtUbt6+3lQZy6KJC0Vtc9dlT+Kf/7cPN6vaeZvntXiokgyUlHHgVtmFPDwzVN4ekMNv3h7v9dxRGSEqajjxNeWTuGWGQX8/W+38+c9WlwUSSYq6jgRCBg/qJjLxHEZPPTEBg5rcVEkaaio40hWWpiV95XT09vPA7+u1OKiSJJQUceZyXmj+OE989h6pIVvP7NFi4siSUBFHYduurqAb9w8lWc3HuaRF7dxor3b60giEkMhrwPIpfmvN15FbUsnv/zzflZXHuKLCybyXxZOYmxmitfRRGSYWSzeOpeXl7vKysphf1052866Vn702i5+u+UoGeGgClskTplZlXOu/JyPqagTw666Vn70+m5eev8I6dHC/rIKWyRuqKiTiApbJD6pqJPQrrpWfvz6bl6MFvZ910/kywuvZNyoVK+jicg5qKiT2O76Vn70mgpbxO9U1MLu+sgI+4XNkcK+9/oJ3L9wkgpbxCdU1HLawMJOCwW5b4EKW8QPVNRylt31bTz6+i5e2HyE1FCQ+66fwJcXTSJXhS3iCRW1nJcKW8QfVNRyUXsa2nj09d08v+kwqaHoHLYKW2TEqKhl0FTYIt64rKI2s58DtwP1zrlZg/mGKur4tzda2M9tOkxKKMC9103g/kWTyctSYYvEwuUW9SKgDfiVijr5qLBFRsZlT32Y2UTgJRV18trb0Maja3fz3MZIYX/h4xO4f/Ek8rPSvI4mkhBGpKjN7H7gfoArrrii7MCBA5eWVnxtX2M7P359lwpbZJhpRC3Dbl9jO4++vptnN9YQDgb4wnUTeECFLXLJLlTUusKLXJIrczP5fsVcXvvmEm6fM55fvL2fhd9by3df2kZ9a6fX8UQSikbUMiz2N7bz6NrdPLvxMKGAcee88dw6u4gbJueSEtJ4QORiLveojyeBJUAuUAd8xzn3swt9jYo6ee1vbOcnb+zht1uO0tbVS1ZqiJuuzmf5rCIWT80jPSXodUQRX9IJLzLiunr7eGt3Iy9vqeXV7XU0newhPRxkybQ8ls8qZOn0fLLSwl7HFPGNCxW1Lm4rMZEaCrJ0egFLpxfQ29fP+n3Hebn6KL/fWsfL1bWkBAN8Ykouy2cVcsvVBYzRFWhEzksjahlR/f2ODQdP8HJ1LWuqaznc1EEwYFw/aRzLZhWybGaBjhyRpKSpD/El5xxbDjezJlraexvbMYPyCWNYPquIZTMLKBmT4XVMkRGhohbfc86xs66Nl6uPsqa6lg9qWwGYU5LD8lmF3DqriCtzMz1OKRI7KmqJO/sa26Mj7aNsrmkGYFpBVqS0ZxcyrSALM/M4pcjwUVFLXDvc1MHvo9Mj7x04jnORE26WzSzk1lmFzCnJUWlL3FNRS8Kob+3k1W11rKmu5e09x+jrdxSPTmfZzEKWzyqkbMIYggGVtsQfFbUkpKaT3by6rY7fb63lzV2NdPf2kzsqlWUzC1g+q5DrJo0jHNRZkRIfVNSS8Nq6enn9g3rWVB9l7QcNdPT0MTojzM1XF7B8ZiGfmJJLWlhnRYp/qaglqXT29LFuZwNrqmv5w/Y6Wjt7GZUa4sbp+dwyo4BFU3IZnaETbMRfdGaiJJW0cJBlMwtZNrOQ7t5+3t7TyJrqWl7ZVseLm48QMJhXOprFU/NZPC2P2cU5mtcWX9OIWpJGX79jc00Tb+xoYN3OBt6vacI5GJMRZtHUPBZPzWPhlDxdZkw8oakPkXM43t7NH3c1sG5HA2/uaqCxrRuA2cU5LJ6ax+JpecwvHU1IC5IyAlTUIhfR3+/YeqSFdTvrWbezgQ0Hm+jrd2SlhVg4JTdS3FPzKczRPiQSGypqkSFq7ujhrd2NrItOk9S2RK5aM70wK1raeZRPHKuLIsiwUVGLXAbnHDvqWk+X9nv7j9PT58hICbJgci6Lp+WxZGoepWO1gZRcOhW1yDBq6+rlz3uOsW5nPW/saKDmRAcAk/IyWTw1jyXT8vn4lWN13LYMiYpaJEacc+xtbGfdjgbe2NnAO3uP0d3bT2oowHWTxrFkWmSa5MrcTO1HIhekohYZIR3dfazfd4w3djTw5s4G9ja2A1A6Np0lU/NZPDWP6yePIzNVpzDIR6moRTxy8NhJ1u1qYN2Oet7ec4yT3X2Eg8bHJo6NjrbzmVowSqNtUVGL+EFXbx9V+0/wxs7Isds76iIXRyjKSWP+FaMpyE6jMDuNwpy0j9zWXHdyUFGL+NDR5o7TR5LsqGulrrmT9u6+s56Xkx6mMDuNgpw0CrNTT98uyPqw1MdlphDQafBxTUUtEidaO3uoa+mktrmL2pbO6O3Oj9xubOui/4xf23DQyM9KoyA79awRuUbn8UGbMonEiay0MFlpYa7Kzzrvc3r7+mlo66K2uZO6lq5Igbd0Uhct9A9qI8d8a3SeOFTUInEmFAxQlJNOUU76BZ93sdH5B0dbLjo6zx2VSk56mOz0cOTftBA5GWGy08Jn3B8mLRzQomiMqKhFEtTQR+eRIq9r7To9Oj9w7CQtnT00d/Rw8hwj9IFSggGy00Nkp0UK/CPlfkapRz4Pnb6dlRbS5lcXoKIWSWKDHZ0D9PT109LRQ0tnL80dPbR0RAq8pbOHlo7e07dPP3aym0PHT57+vPfMofsZRqWGyE4LnVHyH5b6qc9PPzbgvoyUYEKP5lXUIjIo4WCAcaNSGTdq6Pt1O+fo6OmLlvbZRX/q/oFFf+j4SVqjfxTaunov+PqhgEUK/lTRnzFqP1X+A0f3A5+TGvL3IquKWkRizszISAmRkRKiKGfoX9/b109rZ+9Zo/fzjepbOno42txBS2cvLR09dPX2X/D1U0OBAQX+0TI/52h+wP1ZaeGYXyFIRS0ivhcKBhiTmcKYzEu71mVnT985y/zUVE7LGSP7Y+3d7G1sP/143yCmbXLSw4wfncbqBxdcUsYLUVGLSMJLCwdJCwe5wLrqeTnnaO/u+7DMT35Y7gNH8y2dPYSDsRlZq6hFRC7AzBiVGmJUaojxXHzRNRZ0PIyIiM+pqEVEfE5FLSLicypqERGfU1GLiPicilpExOdU1CIiPqeiFhHxuZhc4cXMGoADl/jluUDjMMYZLso1NMo1NMo1NImYa4JzLu9cD8SkqC+HmVWe73I0XlKuoVGuoVGuoUm2XJr6EBHxORW1iIjP+bGoV3od4DyUa2iUa2iUa2iSKpfv5qhFROSj/DiiFhGRAVTUIiI+54uiNrNSM1trZtvMbKuZfd3rTABmlmZm75rZ5miuR7zONJCZBc1so5m95HWWgcxsv5ltMbNNZlbpdZ5TzGy0mT1lZh+Y2XYzu94HmaZFf06nPlrM7GGvcwGY2Tei/99Xm9mTZpbmdSYAM/t6NNNWL39WZvZzM6s3s+oB9401s1fNbFf03zHD8b18UdRAL/BN59wM4DrgITOb4XEmgC5gqXNuLjAPWG5m13kb6SO+Dmz3OsR53Oicm+ezY13/BVjjnJsOzMUHPzvn3I7oz2keUAacBJ71NhWYWTHwNaDcOTcLCAL3eJsKzGwW8GXgWiL/DW83s6s8ivMLYPkZ9/0V8JpzbgrwWvTzy+aLonbOHXXObYjebiXyC1TsbSpwEW3RT8PRD1+svppZCfAp4KdeZ4kHZpYDLAJ+BuCc63bONXka6mw3AXucc5d6Vu9wCwHpZhYCMoAjHucBuBpY75w76ZzrBdYBn/EiiHPuTeD4GXffCfwyevuXwF3D8b18UdQDmdlEYD6w3uMowOnphU1APfCqc84XuYAfAt8C+j3OcS4OeMXMqszsfq/DRF0JNAD/Hp0u+qmZZXod6gz3AE96HQLAOXcY+CfgIHAUaHbOveJtKgCqgYVmNs7MMoDbgFKPMw1U4Jw7Gr1dCxQMx4v6qqjNbBTwNPCwc67F6zwAzrm+6NvSEuDa6FsvT5nZ7UC9c67K6yzn8Qnn3DXArUSmsRZ5HYjI6PAa4CfOuflAO8P0tnQ4mFkKcAew2ussANG51TuJ/IEbD2Sa2Re8TQXOue3A94BXgDXAJqDPy0zn4yLHPg/LO3DfFLWZhYmU9OPOuWe8znOm6NvktZw9J+WFG4A7zGw/8BtgqZk95m2kD0VHYzjn6onMt17rbSIAaoCaAe+IniJS3H5xK7DBOVfndZCom4F9zrkG51wP8AywwONMADjnfuacK3POLQJOADu9zjRAnZkVAUT/rR+OF/VFUZuZEZk73O6c+4HXeU4xszwzGx29nQ7cAnzgaSjAOfdt51yJc24ikbfLrzvnPB/tAJhZppllnboNfJLI21VPOedqgUNmNi16103ANg8jnenz+GTaI+ogcJ2ZZUR/P2/CB4uvAGaWH/33CiLz0094m+gjXgC+GL39ReD54XjR0HC8yDC4AbgX2BKdDwb4a+fc77yLBEAR8EszCxL5o7bKOeerQ+F8qAB4NvK7TQh4wjm3xttIp30VeDw6zbAX+M8e5wFO/0G7BXjA6yynOOfWm9lTwAYiR2VtxD+nbT9tZuOAHuAhrxaFzexJYAmQa2Y1wHeA/wWsMrMvEdnquWJYvpdOIRcR8TdfTH2IiMj5qahFRHxORS0i4nMqahERn1NRi4j4nIpaRMTnVNQiIj73/wGilljbjvS1HAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "elbowPlot = pd.DataFrame(dist)\n",
        "elbowPlot.rename(columns={0: \"Inner cluster distance\"}, inplace=True)\n",
        "elbowPlot[\"Number of Clusters\"] = np.arange(2, 11)\n",
        "\n",
        "plt.plot(elbowPlot[\"Number of Clusters\"], elbowPlot[\"Inner cluster distance\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the elbow method, 3 or 4 wine types seems to be best. This is because at these values for k, the graph becomes less steep and starts to flatten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x2eiPRk_Wrp"
      },
      "source": [
        "#### Clustering using DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laqjJdOi_Wrp"
      },
      "source": [
        "Another clustering algorithm we learned about last week is DBSCAN.\n",
        "\n",
        "To create the model, we need to decide our radius factor, `eps`, which tells us how large we think our clusters will be, and the minimum number of samples we want included in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkx--XsM_Wrp",
        "outputId": "3fd475ae-dba0-424e-94d8-eb8e4f0b9f16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DBSCAN(eps=40, min_samples=20)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "cluster_DBSCAN = DBSCAN(eps=40, min_samples=20)\n",
        "cluster_DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELVwSwqD_Wrq"
      },
      "source": [
        "Then we fit the DBSCAN algorithm to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dpsPBwt_Wrq",
        "outputId": "e37982c6-291e-4aec-87e6-18ec63692349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DBSCAN(eps=40, min_samples=20)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_DBSCAN.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQ92Oea_Wrr"
      },
      "source": [
        "And now we can predict the cluster of data points using our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "tT2mHUUu_Wrs"
      },
      "outputs": [],
      "source": [
        "DBSCAN_predict = cluster_DBSCAN.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWOdZq4_Wrs"
      },
      "source": [
        "Unlike k-means, DBSCAN tries to figure out the optimal number of clusters on its own.\n",
        "\n",
        "How many clusters do we have in this fitted model? And what are they?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIaegMdi_Wrt",
        "outputId": "a83fb7ef-da6e-47da-c89b-c0e5ef0c18a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "(array([-1,  0,  1]), array([99, 42, 37]))\n"
          ]
        }
      ],
      "source": [
        "print(len(set(DBSCAN_predict)))\n",
        "print(np.unique(DBSCAN_predict, return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1YZ7cIH_Wrt"
      },
      "source": [
        "So here we have two groups (0 and 1) with some outliers (-1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRQlkn5s_Wrt"
      },
      "source": [
        "#### Why are we seeing different results using k-means and DBSCAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgCdXtmh_Wrt"
      },
      "source": [
        "We use TSNE, another dimension reduction algorithm besides Principal Component Analysis, to visualize our data in two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9sfnx8fq_Wrt",
        "outputId": "ee0d8d7a-a495-4440-b349-5a6605e0cf71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fee3b40cc40>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
        "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=wine.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQTfykyp_Wru"
      },
      "source": [
        "It looks like two of the wines are hard to differentiate, and the third is still not that different from the other two.\n",
        "\n",
        "Therefore, it is hard to decide how many clusters is best. So different algorithms with their different assumptions will give us different answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GCmgQR7_Wru"
      },
      "source": [
        "## Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kMRNScr_Wru"
      },
      "source": [
        "It's time for a competition. \n",
        "\n",
        "You should have gotten a dataset on forest fires in Portugal taken from `kaggle.com`. We are going to try and predict the amount of area burned using variables like wind, rain, day of the week, and month of the year.\n",
        "\n",
        "Try different algorithms for regression tasks and tune their hyperparameters if necessary. The people with the 5 lowest mean square errors will receive a prize!\n",
        "\n",
        "*Hint*: The outcome variable, area burned, is very right-skewed, or most values are small and just a few are larger. Fitting models using the log of the outcome may help. Then when you predict values, exponentiate them to get them on the original scale. Use the `np.log()` and `np.exp()` functions to take the natural log/raise to the power of e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "asFqMj9K_Wru",
        "outputId": "243ae368-5f5b-4881-9f12-13ae668df30a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH',\n",
              "       'wind', 'rain', 'area'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fires = pd.read_csv(\"forestfires.csv\")\n",
        "fires.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nYMP-3L_Wru"
      },
      "source": [
        "A description of each variable is below:\n",
        "\n",
        "*   `X` is the x-coordinate of the fire on a map of the Portugal park (not very important)\n",
        "*   `Y` is the y-coordinate of the fire on a map of the Portugal park (not very important)\n",
        "*   `month` is the month the fire occured in\n",
        "*   `day` is the day of the week the fire occured on\n",
        "*   `FFMC` stands for \"Fine Fuel Moisture Code\" and indicates the moisture levels among the small leaves in the forest.\n",
        "*   `DMC` stands for \"Duff Moisture Code\" and indicates the moisture levels among decomposed organic material.\n",
        "*   `DC` stands for \"Drought Code\" and indicates how dry the deeper soil is.\n",
        "*   `ISI` stands for \"Initial Spread Index\" and takes into account the moisture of fuels for fire and windspeed to determine how likely things are to be spread around in the forest.\n",
        "*   `temp` is the temperature in Celsius during the fire.\n",
        "*   `RH` is the relative humidity in percentage terms.\n",
        "*   `wind` is the windspeed in $km/h$ during the fire.\n",
        "*   `rain` is the amount of rain in $mm/m^2$ during the fire.\n",
        "*   `area`, our outcome variable, is the amount of area burned by the fire in hectares.\n",
        "\n",
        "*Note*: a lot of these variables are correlated with one another. So it may be better to choose a subset of them when fitting models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QonX2cM1_Wrw"
      },
      "source": [
        "Then `scikit-learn` needs our outcome variable in a separate dataframe from our predictors. We create those two objects now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "aeLCaGTN_Wrw"
      },
      "outputs": [],
      "source": [
        "Y = fires[\"area\"]\n",
        "X = fires.drop(columns=\"area\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9vP5-yz_Wrx"
      },
      "source": [
        "Now we create our training and testing datasets. By making `random_state=0` we assure that everyone uses the same training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "mrhAYrTj_Wrx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3,  random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5nkFstu_Wrx"
      },
      "source": [
        "OK take it away! Have fun creating your models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6e5r4Un57I2-"
      ],
      "name": "AppliedMachineLearning.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
