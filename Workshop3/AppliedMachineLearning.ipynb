{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JPYaq5eWQ4n"
      },
      "source": [
        "# Applied Machine Learning\n",
        "### Workshop 3 of DASIL's series on \"Data Science with Python\"\n",
        "### Created by Martin Pollack, Yusen He, and Declan O'Reilly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FGuCgcWQ4p"
      },
      "source": [
        "In this Jupyter notebook we will teach you how to fit the machine learning models we talked about last week in Python using the `scikit-learn` package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of our example datasets come from the `datasets` sub-package within `scikit-learn`. So we import them now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "UiT01kfZWQ4q"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEOwnQpUWQ4t"
      },
      "source": [
        "## Supervised Learning - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHTB5d26lQO"
      },
      "source": [
        "#### Dataset introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7L_N9Q6WQ4u"
      },
      "source": [
        "Remember that in a regression problem the outcome variable is numeric and continuous. However, the predictor variables can either be continuous or discrete.\n",
        "\n",
        "An example of a regression problem can be found in the diabetes dataset within sklearn. Our outcome is a quantitative measure of disease progression that takes on numbers between 25 and 346."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "3ffUxHDLWQ4v"
      },
      "outputs": [],
      "source": [
        "diabetes = datasets.load_diabetes(as_frame=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`diabetes` is a special scikit-learn dataset. Its `target` field is a pandas Series with our continuous outcome variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      151.0\n",
              "1       75.0\n",
              "2      141.0\n",
              "3      206.0\n",
              "4      135.0\n",
              "       ...  \n",
              "437    178.0\n",
              "438    104.0\n",
              "439    132.0\n",
              "440    220.0\n",
              "441     57.0\n",
              "Name: target, Length: 442, dtype: float64"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Re8w01hY86c"
      },
      "source": [
        "And its `data` field is a pandas Dataframe of all of our potential predictor variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "uZND0mNpYxjS",
        "outputId": "25c1b667-da38-44c5-8e13-bf5e77b9b2d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>0.044485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017282</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.015491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.003064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age       sex       bmi        bp        s1        s2        s3  \\\n",
              "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
              "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
              "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
              "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
              "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
              "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
              "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
              "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
              "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
              "\n",
              "           s4        s5        s6  \n",
              "0   -0.002592  0.019908 -0.017646  \n",
              "1   -0.039493 -0.068330 -0.092204  \n",
              "2   -0.002592  0.002864 -0.025930  \n",
              "3    0.034309  0.022692 -0.009362  \n",
              "4   -0.002592 -0.031991 -0.046641  \n",
              "..        ...       ...       ...  \n",
              "437 -0.002592  0.031193  0.007207  \n",
              "438  0.034309 -0.018118  0.044485  \n",
              "439 -0.011080 -0.046879  0.015491  \n",
              "440  0.026560  0.044528 -0.025930  \n",
              "441 -0.039493 -0.004220  0.003064  \n",
              "\n",
              "[442 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diabetes.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make things easier, we will just rename our `target` to `Y` and our predictors to `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = diabetes.target\n",
        "X = diabetes.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amRvAyBZaNez"
      },
      "source": [
        "#### Splitting the dataset into Training and Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ro8jKDFpwSh"
      },
      "source": [
        "To evaluate the performance of our models, we need to randomly split our features dataset `X` and outcomes `Y` into a training set `X_train`/`y_train` and testing set `X_test`/`Y_test`. Randomly, we will use some observations to train our models and others see how our model generalizes to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40iJCmxjcLVC"
      },
      "source": [
        "The `sklearn.model_selection.train_test_split()` does the random split for the training and testing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VADJGmpq-96"
      },
      "source": [
        "The `test_size` option controls proportion of the observations in our original data that we want to include in our test dataset. The `random_state` parameter controls the random selection process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NZBrjZyeaTxa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "WQ3sh-bKq0ko",
        "outputId": "991c2d8d-fe61-40f0-8257-303c0601b93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(442, 10)\n",
            "(331, 10)\n",
            "(111, 10)\n",
            "(442,)\n",
            "(331,)\n",
            "(111,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(Y.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ntU4ger8wA"
      },
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXs3btQvjH_"
      },
      "source": [
        "Let's fit our first machine learning model! We are going to first fit a linear regression, which is the simplest model we will consider since it has no hyperparameters.\n",
        "\n",
        "At a minimum this process takes two steps.\n",
        "\n",
        "First we create the model object, specifying any hyperparameters (but there are none for now). In this case we want to use the `LinearRegression` object from `sklearn.linear_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "0ArrMz1IthGn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor_LinReg = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0zkZimvm2Q"
      },
      "source": [
        "Second we have to actually fit the linear regression model using our training data. This is done by calling the `fit()` method on our model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYPHVBJvsnB",
        "outputId": "a01f2f63-8b31-4037-9258-29a30220fa54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regressor_LinReg.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQlOWLImv3tr"
      },
      "source": [
        "We can now make predictions for our test dataset using our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "eJt3Ymrsv6iI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.57669262e-02,  5.99308847e-01,  9.48596891e-02,  7.29213466e-01,\n",
              "       -2.37785294e-02,  7.09241459e-01, -1.26333961e-01, -1.35137497e-01,\n",
              "        2.40447123e-01,  2.47922806e-01,  6.67822925e-01,  8.90155147e-01,\n",
              "        8.65392259e-01,  1.20741265e+00, -1.39266813e-01, -4.55980090e-02,\n",
              "       -1.60287546e-01,  6.03321854e-01, -6.99793977e-03,  3.68452928e-01,\n",
              "        1.01018224e+00,  8.31797196e-01,  8.80450312e-01, -3.62284304e-01,\n",
              "        1.19675501e-01,  4.58547132e-01,  8.22724624e-01, -2.35685673e-01,\n",
              "        8.12243161e-01,  3.51449417e-02,  3.43841368e-01,  7.94160681e-01,\n",
              "        3.44708500e-01,  7.13942039e-01,  9.23532343e-01,  1.20630428e+00,\n",
              "        7.32405817e-01, -6.24258954e-01,  1.12793861e+00,  1.77731769e-01,\n",
              "        4.64616161e-01,  9.01784607e-02,  7.61898235e-01,  1.14874022e-01,\n",
              "        6.68978028e-01,  3.46926458e-02, -1.48343376e-02,  9.93999404e-01,\n",
              "        1.09601809e+00,  1.08368074e+00,  8.53826056e-01, -8.71007278e-02,\n",
              "        1.05198136e+00,  6.95932399e-02,  7.33513030e-01,  9.99155126e-01,\n",
              "        5.98347618e-01,  7.86781002e-01,  9.56102615e-01,  9.07706062e-01,\n",
              "        5.82424454e-01,  9.59607569e-01,  3.16015607e-01,  3.98638978e-01,\n",
              "        7.93289138e-01,  7.45344469e-01,  1.78139797e-01, -1.15134316e-02,\n",
              "        1.96030719e-01,  1.09493792e+00,  1.21749977e+00, -1.77843939e-01,\n",
              "        9.04771261e-01,  8.12063260e-01,  8.76933074e-01,  9.03685033e-01,\n",
              "        8.55128986e-01,  1.61501951e-01,  2.73029339e-01,  7.84550045e-01,\n",
              "        9.23214557e-01,  8.31750142e-01,  7.35950231e-01,  8.28487073e-02,\n",
              "        4.20267929e-01,  6.97793501e-01,  1.42544134e-03,  1.58229501e-01,\n",
              "       -6.24007164e-02, -4.54378098e-02,  8.22734919e-01,  9.74789643e-01,\n",
              "        8.20072739e-01,  7.36345371e-01,  6.57369063e-01,  6.71202216e-03,\n",
              "        2.56402219e-01,  7.10248764e-01, -3.58398211e-02,  5.39646738e-01,\n",
              "        5.46474117e-01,  1.20109634e+00,  7.72748964e-01,  4.03004071e-01,\n",
              "        9.26267532e-01,  6.20956394e-01,  6.99880193e-01,  4.14482460e-01,\n",
              "        1.06782216e+00,  6.57947422e-01,  1.12992804e+00,  5.53692576e-02,\n",
              "        4.37064019e-01,  8.42275485e-01,  1.08135846e+00,  1.20753038e+00,\n",
              "        7.08174815e-01,  1.51258514e-01,  9.59712214e-01,  8.82304855e-01,\n",
              "        8.94899375e-01,  1.02837867e+00,  1.13527802e+00,  1.16979915e+00,\n",
              "        2.57658526e-01,  1.15554138e+00,  9.51294125e-01,  8.62366479e-01,\n",
              "        1.02347539e+00,  9.58972299e-01,  1.09932814e+00,  1.02903112e+00,\n",
              "        1.15873426e+00,  8.75486444e-01,  8.67928843e-02,  8.61197121e-01,\n",
              "        9.63185954e-01,  4.05695460e-01,  2.30957912e-01,  9.27665277e-01,\n",
              "        1.06247322e+00, -3.37910935e-01,  1.33629049e+00, -8.57524989e-02,\n",
              "        8.72790591e-01,  1.68568002e-01,  1.02006416e+00,  1.45942417e+00,\n",
              "        8.26633602e-01,  1.00273488e+00,  1.06051182e+00,  1.02613562e+00,\n",
              "        8.76273950e-01,  6.21990888e-01, -1.31539975e-01,  9.71379693e-01,\n",
              "        6.95182180e-01,  1.16829728e+00,  1.06834420e+00,  6.94394780e-01,\n",
              "       -3.18377228e-01,  8.66431372e-01,  7.82864718e-01,  8.14101339e-01,\n",
              "        5.99967823e-01,  9.82527689e-01,  7.64265081e-01,  4.15532504e-01,\n",
              "        8.54909786e-01,  8.54969758e-01,  1.63108288e+00])"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LinReg_pred = regressor_LinReg.predict(X_test)\n",
        "LinReg_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also, we can use the `score()` method to get a quick idea of how our model did. For regression, the evaluation metric used is $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LinReg_pred.score(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_xHJYlKcK-"
      },
      "source": [
        "#### Artificial Neural Network for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhMzhu83KcK_"
      },
      "source": [
        "Now we are going to fit a much more complicated model to our data: an artificial neural network (ANN). ANNs can be very accurate; however, they have lots of hyperparameters that are hard to get right.\n",
        "\n",
        "We start by seeing what possible hyperparameters an ANN has and what their default values are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.0001,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (100,),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.001,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 200,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': None,\n",
              " 'shuffle': True,\n",
              " 'solver': 'adam',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': False,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "MLPRegressor().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can set up an ANN model, this time specifying specific values for hyperparameters like the number of hidden layers. All hyperparameters that we don't specify values for will have their default values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u1IhG5bQKcK_"
      },
      "outputs": [],
      "source": [
        "regressor_ANN_default = MLPRegressor(hidden_layer_sizes=(40,1), solver='lbfgs', max_iter=2000, learning_rate_init=0.000001, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we fit this specific model with the `fit()` method and evaluate it with `score()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.34888647829370967"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regressor_ANN_default.fit(X_train, Y_train)\n",
        "regressor_ANN_default.score(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r27sd4IPZsW"
      },
      "source": [
        "So we just fit an ANN with one set of hyperparameters.\n",
        "\n",
        "But our choice of hyperparameters is going to have a big effect on how well our model does. Thus, ideally we want to try multiple combinations of hyperparameters and then pick the best one.\n",
        "\n",
        "The first method we use for \"hyperparameter tuning\" is the `GridSearchCV()` function. We give it specific values of hyperparameters we want to be considered. These hyperparameters are detailed using a dictionary of lists.\n",
        "\n",
        "Then when we fit our `GridSearchCV` object, we will actually be fitting many ANNs at once, one for each combination of hyperparameters. As part of the fitting process, Python does cross validation (CV) on each model and picks the best combination based on the model with the best overall evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R-sVlyILri3",
        "outputId": "dde81c53-15b8-4282-8831-b95055043d85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=MLPRegressor(max_iter=1000, solver='lbfgs'),\n",
              "             param_grid={'hidden_layer_sizes': [(35, 1), (40, 1), (5, 1),\n",
              "                                                (200, 1)],\n",
              "                         'learning_rate_init': [0.0001, 1e-06]})"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "ANN = MLPRegressor(solver='lbfgs', max_iter=1000)\n",
        "parameters = {'learning_rate_init':[0.0001, 0.000001], 'hidden_layer_sizes':[(35,1), (40,1), (5,1), (200,1)]}\n",
        "regressor_ANN_tuned_grid = GridSearchCV(ANN, parameters)\n",
        "regressor_ANN_tuned_grid.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see all the results, access the `cv_results_` field of our `GridSearchCV` object. And to see the best combination, access the `best_estimator_` field of the same object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mean_fit_time': array([0.03844299, 0.02219429, 0.02072473, 0.02152014, 0.00903125,\n",
            "       0.01142516, 0.05343785, 0.041364  ]), 'std_fit_time': array([0.02765362, 0.00530154, 0.0065149 , 0.01145082, 0.00322381,\n",
            "       0.0044473 , 0.01545191, 0.01168147]), 'mean_score_time': array([0.00517645, 0.00561886, 0.00328107, 0.00436711, 0.00256758,\n",
            "       0.0037962 , 0.00395713, 0.00480256]), 'std_score_time': array([0.00125804, 0.00429571, 0.00074553, 0.00233976, 0.00048171,\n",
            "       0.00169874, 0.00127152, 0.00138232]), 'param_hidden_layer_sizes': masked_array(data=[(35, 1), (35, 1), (40, 1), (40, 1), (5, 1), (5, 1),\n",
            "                   (200, 1), (200, 1)],\n",
            "             mask=[False, False, False, False, False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_learning_rate_init': masked_array(data=[0.0001, 1e-06, 0.0001, 1e-06, 0.0001, 1e-06, 0.0001,\n",
            "                   1e-06],\n",
            "             mask=[False, False, False, False, False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'hidden_layer_sizes': (35, 1), 'learning_rate_init': 0.0001}, {'hidden_layer_sizes': (35, 1), 'learning_rate_init': 1e-06}, {'hidden_layer_sizes': (40, 1), 'learning_rate_init': 0.0001}, {'hidden_layer_sizes': (40, 1), 'learning_rate_init': 1e-06}, {'hidden_layer_sizes': (5, 1), 'learning_rate_init': 0.0001}, {'hidden_layer_sizes': (5, 1), 'learning_rate_init': 1e-06}, {'hidden_layer_sizes': (200, 1), 'learning_rate_init': 0.0001}, {'hidden_layer_sizes': (200, 1), 'learning_rate_init': 1e-06}], 'split0_test_score': array([-0.07370956, -0.07370956, -0.07370956, -0.07370956, -0.07370956,\n",
            "       -0.07370965, -0.07370956, -0.07370956]), 'split1_test_score': array([-0.0274257, -0.0274257, -0.0274257, -0.0274257, -0.0274257,\n",
            "       -0.0274257, -0.0274257, -0.0274257]), 'split2_test_score': array([-0.01067585, -0.01067585, -0.01067585, -0.01067585, -0.01067585,\n",
            "       -0.01067585, -0.01067585, -0.01067585]), 'split3_test_score': array([-0.00275601, -0.002756  , -0.00275601, -0.0025136 , -0.02822308,\n",
            "       -0.00275601, -0.00275601, -0.00275601]), 'split4_test_score': array([-0.00626374, -0.00626374, -0.00626374, -0.00626374, -0.00626374,\n",
            "       -0.00626374, -0.00626374, -0.00626374]), 'mean_test_score': array([-0.02416617, -0.02416617, -0.02416617, -0.02411769, -0.02925959,\n",
            "       -0.02416619, -0.02416617, -0.02416617]), 'std_test_score': array([0.02617653, 0.02617653, 0.02617653, 0.02621633, 0.0238931 ,\n",
            "       0.02617656, 0.02617653, 0.02617653]), 'rank_test_score': array([4, 2, 3, 1, 8, 7, 5, 6], dtype=int32)}\n",
            "MLPRegressor(hidden_layer_sizes=(40, 1), learning_rate_init=1e-06,\n",
            "             max_iter=1000, solver='lbfgs')\n",
            "-0.024117691797310804\n"
          ]
        }
      ],
      "source": [
        "print(regressor_ANN_tuned_grid.cv_results_)\n",
        "print(regressor_ANN_tuned_grid.best_estimator_)\n",
        "print(regressor_ANN_tuned_grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we can what are model predicts using the `predict()` method on our model object. Although we trained multiple ANNs at once, it will by default give us predictions from the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKeB6KumRoQ0",
        "outputId": "6d8cfc95-6088-46f2-a761-0ae13a9cb017"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407, 0.6281407,\n",
              "       0.6281407, 0.6281407, 0.6281407])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ANN_pred = regressor_ANN_tuned_grid.predict(X_test)\n",
        "ANN_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbtu3t5bwXWf"
      },
      "source": [
        "#### Performance Assessment using Testing Data\n",
        "\n",
        "We will compare the two models using these four metrics:\n",
        "*   Mean Absolute Error\n",
        "*   Mean Absolute Percentage Error\n",
        "*   Mean Square Error\n",
        "*   Root-Mean-Square Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEexDKqYx3cA"
      },
      "source": [
        "First, let's compute Mean Absolute Error (MAE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr21dcF-wqao",
        "outputId": "54afc4b8-cfe8-423d-930b-3448a8183ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MAE of predictions provided by GLM is :\n",
            "0.1991114631884264\n",
            "The MAE of predictions provided by ANN is :\n",
            "0.4677774839115687\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "MAE_GLM = mean_absolute_error(Y_test, LinReg_pred)\n",
        "MAE_ANN = mean_absolute_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MAE of predictions provided by GLM is :\")\n",
        "print(MAE_GLM)\n",
        "\n",
        "print(\"The MAE of predictions provided by ANN is :\")\n",
        "print(MAE_ANN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZCsUF1EyATg"
      },
      "source": [
        "Second, let's compute Mean Absolute Percentage Error (MAPE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpCMtL9S3U0h",
        "outputId": "2137e069-3202-43ce-f46e-bace10bc679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MAPE of predictions provided by GLM is :\n",
            "394901538525604.1\n",
            "The MAPE of predictions provided by ANN is :\n",
            "1058767434211540.6\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "MAPE_GLM = mean_absolute_percentage_error(Y_test, LinReg_pred)\n",
        "MAPE_ANN = mean_absolute_percentage_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MAPE of predictions provided by GLM is :\")\n",
        "print(MAPE_GLM)\n",
        "\n",
        "print(\"The MAPE of predictions provided by ANN is :\")\n",
        "print(MAPE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2neFZEkyIJh"
      },
      "source": [
        "Third, let's compute Mean Square Error (MSE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v44igycf39JO",
        "outputId": "2984a3ab-a843-45c7-9b65-aa1c1af2ed96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MSE of predictions provided by GLM is :\n",
            "0.06575916846493211\n",
            "The MSE of predictions provided by ANN is :\n",
            "0.2341975238087492\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_GLM = mean_squared_error(Y_test, LinReg_pred)\n",
        "MSE_ANN = mean_squared_error(Y_test, ANN_pred)\n",
        "\n",
        "print(\"The MSE of predictions provided by GLM is :\")\n",
        "print(MSE_GLM)\n",
        "\n",
        "print(\"The MSE of predictions provided by ANN is :\")\n",
        "print(MSE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKo8f8k5yOEh"
      },
      "source": [
        "Last, let's compute Root-Mean-Suqare-Error (RMSE) for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aBlPKXL5oAh",
        "outputId": "ff1d9716-605d-446c-9203-179093d3e60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The RMSE of predictions provided by GLM is :\n",
            "0.2564355054685917\n",
            "The RMSE of predictions provided by ANN is :\n",
            "0.4839395869411276\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "RMSE_GLM = mean_squared_error(Y_test, LinReg_pred, squared=False)\n",
        "RMSE_ANN = mean_squared_error(Y_test, ANN_pred, squared=False)\n",
        "\n",
        "print(\"The RMSE of predictions provided by GLM is :\")\n",
        "print(RMSE_GLM)\n",
        "\n",
        "print(\"The RMSE of predictions provided by ANN is :\")\n",
        "print(RMSE_ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SarBlYm2WQ4x"
      },
      "source": [
        "## Supervised Learning - Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULypkTWU9hEp"
      },
      "source": [
        "#### Dataset Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29YqLUpTWQ4z"
      },
      "source": [
        "Now let's look at a classification problem, where the outcome can only take on 2 or more discrete values. But of course our predictors can be either continuous or discrete.\n",
        "\n",
        "Now we use `scikit-learn`'s breast cancer dataset. Here the outcome can take on a 0, for no breast cancer, or 1, for breast cancer. \n",
        "\n",
        "So in this case we actually have a *binary classification* problem, meaning our category can only take on 2 discrete values. In most binary classification problems, like in this case, the categories are 0 and 1 indiciating the presence or absence of some trait."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Jmrv4VMWQ41"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZau-rgMWQ41",
        "outputId": "b2086757-82cd-4c3a-b6ee-f7b4113dc8d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    357\n",
              "0    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "NU-R05ZjXEbr",
        "outputId": "2142159d-2011-4fb6-ea88-3df509bd5c1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d591af0-f0b4-464a-a007-65a751db1320\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d591af0-f0b4-464a-a007-65a751db1320')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d591af0-f0b4-464a-a007-65a751db1320 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d591af0-f0b4-464a-a007-65a751db1320');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "0                   0.07871  ...        25.380          17.33   \n",
              "1                   0.05667  ...        24.990          23.41   \n",
              "2                   0.05999  ...        23.570          25.53   \n",
              "3                   0.09744  ...        14.910          26.50   \n",
              "4                   0.05883  ...        22.540          16.67   \n",
              "..                      ...  ...           ...            ...   \n",
              "564                 0.05623  ...        25.450          26.40   \n",
              "565                 0.05533  ...        23.690          38.25   \n",
              "566                 0.05648  ...        18.980          34.12   \n",
              "567                 0.07016  ...        25.740          39.42   \n",
              "568                 0.05884  ...         9.456          30.37   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "0             184.60      2019.0           0.16220            0.66560   \n",
              "1             158.80      1956.0           0.12380            0.18660   \n",
              "2             152.50      1709.0           0.14440            0.42450   \n",
              "3              98.87       567.7           0.20980            0.86630   \n",
              "4             152.20      1575.0           0.13740            0.20500   \n",
              "..               ...         ...               ...                ...   \n",
              "564           166.10      2027.0           0.14100            0.21130   \n",
              "565           155.00      1731.0           0.11660            0.19220   \n",
              "566           126.70      1124.0           0.11390            0.30940   \n",
              "567           184.60      1821.0           0.16500            0.86810   \n",
              "568            59.16       268.6           0.08996            0.06444   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "0             0.7119                0.2654          0.4601   \n",
              "1             0.2416                0.1860          0.2750   \n",
              "2             0.4504                0.2430          0.3613   \n",
              "3             0.6869                0.2575          0.6638   \n",
              "4             0.4000                0.1625          0.2364   \n",
              "..               ...                   ...             ...   \n",
              "564           0.4107                0.2216          0.2060   \n",
              "565           0.3215                0.1628          0.2572   \n",
              "566           0.3403                0.1418          0.2218   \n",
              "567           0.9387                0.2650          0.4087   \n",
              "568           0.0000                0.0000          0.2871   \n",
              "\n",
              "     worst fractal dimension  \n",
              "0                    0.11890  \n",
              "1                    0.08902  \n",
              "2                    0.08758  \n",
              "3                    0.17300  \n",
              "4                    0.07678  \n",
              "..                       ...  \n",
              "564                  0.07115  \n",
              "565                  0.06637  \n",
              "566                  0.07820  \n",
              "567                  0.12400  \n",
              "568                  0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "breast_cancer.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnasUbL_9xOg"
      },
      "source": [
        "#### Define the predictor variable set and a target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQALRlB997O7"
      },
      "source": [
        "For the regression tasks, a feature set `X` and a target set `y` need to be defined first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "eWj12b_r98X8",
        "outputId": "edbc548a-c8e4-4437-98ed-638008618ae1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "0                   0.07871  ...        25.380          17.33   \n",
              "1                   0.05667  ...        24.990          23.41   \n",
              "2                   0.05999  ...        23.570          25.53   \n",
              "3                   0.09744  ...        14.910          26.50   \n",
              "4                   0.05883  ...        22.540          16.67   \n",
              "..                      ...  ...           ...            ...   \n",
              "564                 0.05623  ...        25.450          26.40   \n",
              "565                 0.05533  ...        23.690          38.25   \n",
              "566                 0.05648  ...        18.980          34.12   \n",
              "567                 0.07016  ...        25.740          39.42   \n",
              "568                 0.05884  ...         9.456          30.37   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "0             184.60      2019.0           0.16220            0.66560   \n",
              "1             158.80      1956.0           0.12380            0.18660   \n",
              "2             152.50      1709.0           0.14440            0.42450   \n",
              "3              98.87       567.7           0.20980            0.86630   \n",
              "4             152.20      1575.0           0.13740            0.20500   \n",
              "..               ...         ...               ...                ...   \n",
              "564           166.10      2027.0           0.14100            0.21130   \n",
              "565           155.00      1731.0           0.11660            0.19220   \n",
              "566           126.70      1124.0           0.11390            0.30940   \n",
              "567           184.60      1821.0           0.16500            0.86810   \n",
              "568            59.16       268.6           0.08996            0.06444   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "0             0.7119                0.2654          0.4601   \n",
              "1             0.2416                0.1860          0.2750   \n",
              "2             0.4504                0.2430          0.3613   \n",
              "3             0.6869                0.2575          0.6638   \n",
              "4             0.4000                0.1625          0.2364   \n",
              "..               ...                   ...             ...   \n",
              "564           0.4107                0.2216          0.2060   \n",
              "565           0.3215                0.1628          0.2572   \n",
              "566           0.3403                0.1418          0.2218   \n",
              "567           0.9387                0.2650          0.4087   \n",
              "568           0.0000                0.0000          0.2871   \n",
              "\n",
              "     worst fractal dimension  \n",
              "0                    0.11890  \n",
              "1                    0.08902  \n",
              "2                    0.08758  \n",
              "3                    0.17300  \n",
              "4                    0.07678  \n",
              "..                       ...  \n",
              "564                  0.07115  \n",
              "565                  0.06637  \n",
              "566                  0.07820  \n",
              "567                  0.12400  \n",
              "568                  0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = breast_cancer.data\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TkzBmuh6-B6J",
        "outputId": "4d4fef68-0bc3-4f84-8222-ac04c84270f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "564    0\n",
              "565    0\n",
              "566    0\n",
              "567    0\n",
              "568    1\n",
              "Name: target, Length: 569, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y = breast_cancer.target\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eOA-Y_9lcz"
      },
      "source": [
        "#### Split the dataset into training and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89bteG4dHsXY"
      },
      "source": [
        "The `sklearn.model_selection.train_test_split` does the random split for the training and testing dataset.\n",
        "\n",
        "Note: Here, `stratify` means the test set has equal numbers of 0 & 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HU4MmjqLHtaQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, stratify=Y,  random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB6c_jrmIdF4"
      },
      "source": [
        "To evaluate the model performance, we need to randomly split the feature set `X` and the target set `y` into the training set `X_train` & `y_train` and test set `X_test` & `y_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qayqpBTIdp3"
      },
      "source": [
        "The `test_size` option controls the size of the test set. The `random_state` parameter controls the shuffling applied which can be ignored in this work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elJ87AHnKp0t"
      },
      "source": [
        "#### XGBoost for Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first classification model we will try is XGBoost, considered one of the best models out there.\n",
        "\n",
        "Again we start by seeing what hyperparameters we can possibly tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.0,\n",
              " 'criterion': 'friedman_mse',\n",
              " 'init': None,\n",
              " 'learning_rate': 0.1,\n",
              " 'loss': 'deviance',\n",
              " 'max_depth': 3,\n",
              " 'max_features': None,\n",
              " 'max_leaf_nodes': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_iter_no_change': None,\n",
              " 'random_state': None,\n",
              " 'subsample': 1.0,\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GradientBoostingClassifier().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX6uFoxntutW"
      },
      "source": [
        "Now let's create a model with specific hyperparameters. We will name it as `classifier_XGB`.\n",
        "\n",
        "In this case, the score we get is the overall accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apugvOWmtt8G",
        "outputId": "c114e71c-060d-4b81-d856-f4e16f5638df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9949748743718593"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier_XGB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0)\n",
        "classifier_XGB.fit(X_train, Y_train)\n",
        "classifier_XGB.score(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But again we probably want to do some hyperparameter tuning.\n",
        "\n",
        "The second main way to do this is `RandomizedSearchCV()`. Here we give distributions for our hyperparameters instead of specific values. Python will then randomly choose hyperparameters to try based on the given distributions.\n",
        "\n",
        "For example, for XGBoost we will use a normal distribution with mean of 0.5 and standard deviation of 0.1 for the \"minimum impurity decrease\". This means we will mostly try values close to 0.5, but occasionally some further from 0.5. We will then consider a uniform distribution for learning rate between 0 and 1, meaning any number in this range is equally likely to be chosen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "XGB = GradientBoostingClassifier(n_estimators=100)\n",
        "\n",
        "from scipy.stats import norm, uniform\n",
        "distributions = {\"min_impurity_decrease\":norm(loc=0.5, scale=0.1), \"learning_rate\":uniform(loc=0.5, scale=0.5)}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "classifier_XGB_tuned_random = RandomizedSearchCV(XGB, distributions, n_iter=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we fit our `RandomizedSearchCV` object. Since `n_iter` is 10 above, we will grab 10 combinations of hyperparameters from our two distributions. Then we will fit an XGBoost model for each combination, Python choosing the best one for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.766585400431891, 'min_impurity_decrease': 0.5303481813759702}\n",
            "0.9447468354430381\n"
          ]
        }
      ],
      "source": [
        "classifier_XGB_tuned_random.fit(X_train, Y_train)\n",
        "XGB_pred = classifier_XGB_tuned_random.predict(X_test)\n",
        "\n",
        "print(classifier_XGB_tuned_random.best_params_)\n",
        "print(classifier_XGB_tuned_random.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIQhfBLiK0Zn"
      },
      "source": [
        "#### Support Vector Machine for Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp-UWCS8uBY0"
      },
      "source": [
        "Lastly, we will create an SVM model. \n",
        "\n",
        "We will just go back to using `GridSearchCV()` for our hyperparameter tuning. To know what to tune, let's see what the hyperparameters are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-RYcxNYuQRJ",
        "outputId": "af150a16-6fc7-438f-d4f2-3ebddff71109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'C': 1.0,\n",
              " 'break_ties': False,\n",
              " 'cache_size': 200,\n",
              " 'class_weight': None,\n",
              " 'coef0': 0.0,\n",
              " 'decision_function_shape': 'ovr',\n",
              " 'degree': 3,\n",
              " 'gamma': 'scale',\n",
              " 'kernel': 'rbf',\n",
              " 'max_iter': -1,\n",
              " 'probability': False,\n",
              " 'random_state': None,\n",
              " 'shrinking': True,\n",
              " 'tol': 0.001,\n",
              " 'verbose': False}"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "SVC().get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now create and fit our model, leaving `gamma` fixed at `auto` and tuning `C`, which is by far the most important hyperparameter to get right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.001}\n",
            "0.6281645569620252\n"
          ]
        }
      ],
      "source": [
        "SVC_model = SVC(gamma='auto', probability=True)\n",
        "\n",
        "parameters = {\"C\":[0.001, 0.1, 0.5, 1, 1.5]}\n",
        "\n",
        "classifier_SVM_tuned_grid = GridSearchCV(SVC_model, parameters)\n",
        "\n",
        "classifier_SVM_tuned_grid.fit(X_train, Y_train)\n",
        "print(classifier_SVM_tuned_grid.best_params_)\n",
        "print(classifier_SVM_tuned_grid.best_score_)\n",
        "\n",
        "SVM_pred = classifier_SVM_tuned_grid.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1fxh88pvQzj"
      },
      "source": [
        "#### Performance Assessment\n",
        "\n",
        "*   Confusion Matrix\n",
        "*   Accuracy\n",
        "*   Sensitivity\n",
        "*   Specificity\n",
        "*   AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDaIRwwW4BZi"
      },
      "source": [
        "We start by assessing XGBoost:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqsmOh8fz9Gn",
        "outputId": "2ab94478-5e06-4ceb-d426-5e84736da852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix for XGB: \n",
            " [[ 57   7]\n",
            " [  7 100]]\n",
            "Accuracy for XGB:  0.9181286549707602\n",
            "Specificity for SVM:  0.9345794392523364\n",
            "Sensitivity for SVM:  0.890625\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#Compute the confusion matrix\n",
        "cmXGB = confusion_matrix(Y_test,XGB_pred)\n",
        "print('Confusion Matrix for XGB: \\n', cmXGB)\n",
        "\n",
        "#Compute total test cases\n",
        "totalXGB=sum(sum(cmXGB))\n",
        "#####from confusion matrix calculate accuracy\n",
        "accuracyXGB=(cmXGB[0,0]+cmXGB[1,1])/totalXGB\n",
        "print ('Accuracy for XGB: ', accuracyXGB)\n",
        "\n",
        "sensitivityXGB = cmXGB[1,1]/(cmXGB[1,0]+cmXGB[1,1])\n",
        "print('Specificity for XGB: ', sensitivityXGB)\n",
        "\n",
        "specificityXGB = cmXGB[0,0]/(cmXGB[0,0]+cmXGB[0,1])\n",
        "print('Sensitivity for XGB: ', specificityXGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ekaseon1kDu"
      },
      "source": [
        "Then, we compute the AUC. We use the `roc_auc_score` from `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8eqsAS210uf",
        "outputId": "971a77a0-ccbe-4023-dadd-c481cc0fba2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC for XGB:  0.9752482476635513\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#We first compute the probability output from the SVM classifier\n",
        "XGB_prob = classifier_XGB_tuned_random.predict_proba(X_test)\n",
        "\n",
        "#We compute the AUC score\n",
        "aucSVM = roc_auc_score(Y_test,XGB_prob[:,1])\n",
        "print('AUC for XGB: ', aucSVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDaIRwwW4BZi"
      },
      "source": [
        "Now for SVM's prediction, we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqsmOh8fz9Gn",
        "outputId": "2ab94478-5e06-4ceb-d426-5e84736da852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix for SVM: \n",
            " [[  0  64]\n",
            " [  0 107]]\n",
            "Accuracy for SVM:  0.6257309941520468\n",
            "Specificity for SVM:  1.0\n",
            "Sensitivity for SVM:  0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#Compute the confusion matrix\n",
        "cmSVM = confusion_matrix(Y_test,SVM_pred)\n",
        "print('Confusion Matrix for SVM: \\n', cmSVM)\n",
        "\n",
        "#Compute total test cases\n",
        "totalSVM=sum(sum(cmSVM))\n",
        "#####from confusion matrix calculate accuracy\n",
        "accuracySVM=(cmSVM[0,0]+cmSVM[1,1])/totalSVM\n",
        "print ('Accuracy for SVM: ', accuracySVM)\n",
        "\n",
        "sensitivitySVM = cmSVM[1,1]/(cmSVM[1,0]+cmSVM[1,1])\n",
        "print('Specificity for SVM: ', sensitivitySVM)\n",
        "\n",
        "specificitySVM = cmSVM[0,0]/(cmSVM[0,0]+cmSVM[0,1])\n",
        "print('Sensitivity for SVM: ', specificitySVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ekaseon1kDu"
      },
      "source": [
        "Then, we compute the AUC for SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8eqsAS210uf",
        "outputId": "971a77a0-ccbe-4023-dadd-c481cc0fba2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC for SVM:  0.8879964953271028\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#We first compute the probability output from the SVM classifier\n",
        "SVM_prob = classifier_SVM_tuned_grid.predict_proba(X_test)\n",
        "\n",
        "#We compute the AUC score\n",
        "aucSVM = roc_auc_score(Y_test,SVM_prob[:,1])\n",
        "print('AUC for SVM: ', aucSVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX7SnsiDWQ43"
      },
      "source": [
        "## Unsupervised Learning - Clustering (and Dimension Reduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORhQRZbv7Cwc"
      },
      "source": [
        "#### Dataset Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5MeAadHWQ44"
      },
      "source": [
        "Lastly we want to consider an unsupervised learning problem, where we don't actually have an outcome at all, or our data is \"unlabeled.\" Instead of predicting something we just want to find patterns and structure in our data.\n",
        "\n",
        "Our data can be unlabeled for two reasons:\n",
        "\n",
        "â€¢ First, maybe our data does not have well-defined groupings. An example might be a company's customers: there are not clear and distinct groups that we can put people in. \n",
        "\n",
        "â€¢ Second, maybe the label of our data is missing. Suppose you are a wine vendor and you ordered three types of wine from your supplier. When you receive your wine shipment, however, you realize that the labels were not put on. You may want to learn about how the different wine bottles are related to one another to make an educated guess on which is what wine type.\n",
        "\n",
        "Our example below falls in this second case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "SL-PO7LrWQ45"
      },
      "outputs": [],
      "source": [
        "wine = datasets.load_wine(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_SCElBgWQ45",
        "outputId": "2da23c14-9dc9-4f02-c95e-1c6a13c61be5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
              "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
              "       'proanthocyanins', 'color_intensity', 'hue',\n",
              "       'od280/od315_of_diluted_wines', 'proline'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wine.data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4l4wK5z5YUC6",
        "outputId": "ac30d0dd-42ee-4cee-b958-aaf136904852"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c6e1e5b2-5f35-4ca5-9d88-01d41f37c0fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows Ã— 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6e1e5b2-5f35-4ca5-9d88-01d41f37c0fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6e1e5b2-5f35-4ca5-9d88-01d41f37c0fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6e1e5b2-5f35-4ca5-9d88-01d41f37c0fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
              "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
              "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
              "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
              "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
              "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
              "..       ...         ...   ...                ...        ...            ...   \n",
              "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
              "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
              "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
              "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
              "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
              "\n",
              "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
              "0          3.06                  0.28             2.29             5.64  1.04   \n",
              "1          2.76                  0.26             1.28             4.38  1.05   \n",
              "2          3.24                  0.30             2.81             5.68  1.03   \n",
              "3          3.49                  0.24             2.18             7.80  0.86   \n",
              "4          2.69                  0.39             1.82             4.32  1.04   \n",
              "..          ...                   ...              ...              ...   ...   \n",
              "173        0.61                  0.52             1.06             7.70  0.64   \n",
              "174        0.75                  0.43             1.41             7.30  0.70   \n",
              "175        0.69                  0.43             1.35            10.20  0.59   \n",
              "176        0.68                  0.53             1.46             9.30  0.60   \n",
              "177        0.76                  0.56             1.35             9.20  0.61   \n",
              "\n",
              "     od280/od315_of_diluted_wines  proline  \n",
              "0                            3.92   1065.0  \n",
              "1                            3.40   1050.0  \n",
              "2                            3.17   1185.0  \n",
              "3                            3.45   1480.0  \n",
              "4                            2.93    735.0  \n",
              "..                            ...      ...  \n",
              "173                          1.74    740.0  \n",
              "174                          1.56    750.0  \n",
              "175                          1.56    835.0  \n",
              "176                          1.62    840.0  \n",
              "177                          1.60    560.0  \n",
              "\n",
              "[178 rows x 13 columns]"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wine.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5r4Un57I2-"
      },
      "source": [
        "#### Initialize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzcQqq5r-QNs"
      },
      "source": [
        "Initialize the dataset as `X`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "ffGyVj1w-S7v"
      },
      "outputs": [],
      "source": [
        "X= wine.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    71\n",
              "0    59\n",
              "2    48\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wine.target.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0yRsiFz-_K0"
      },
      "source": [
        "#### Clustering using K-Means algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gyXR2eUBcfO"
      },
      "source": [
        "##### Intialize the K-Means Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u22oqn0-IRO"
      },
      "source": [
        "The K-Mean algorithm is included in the Scikit-leanr library. Define the number of clusters by `n_clusters` and random initialization state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN01QBb4-Pdt",
        "outputId": "bee71b5a-38c7-4edd-d72c-045d5c1333ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=3, random_state=0)"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "#Here we set n=3 in this workshop\n",
        "cluster_KMeans = KMeans(n_clusters=3, random_state=0)\n",
        "cluster_KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGumzwgl-1xt"
      },
      "source": [
        "##### Fit and predict using K-Mean algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DJRbWuJ_cBG"
      },
      "source": [
        "Fit the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_ch__Ce-1Gx",
        "outputId": "63a9b541-9617-4f37-d5d1-7bde072c6da5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=3, random_state=0)"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_KMeans.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icvv8h9z_Oh8"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBqEwxW_hoE",
        "outputId": "54343e0c-2609-4edb-8a6a-28e6810c4575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 2, 0,\n",
              "       2, 2, 0, 0, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0,\n",
              "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
              "       2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 0,\n",
              "       0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0,\n",
              "       0, 2], dtype=int32)"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_labels = cluster_KMeans.predict(X)\n",
        "cluster_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEL2KmZH1e3y"
      },
      "source": [
        "Compute inner cluster distances (Euclidean distances) within all clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLcypDGJ1kSZ",
        "outputId": "502808f9-3700-4382-ca85-d6dc43cd558c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2370689.686782968"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_KMeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2VQS91NBUJN"
      },
      "source": [
        "##### Find the BEST k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcA2Ed-z1toI"
      },
      "source": [
        "Now, let's try different numbers of k to see how cluster centers change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "yVW1JGmP1s0P"
      },
      "outputs": [],
      "source": [
        "#Store the sum of inner cluster distances in a list and name is as dist\n",
        "dist=[]\n",
        "\n",
        "for i in range(2,11):\n",
        "  #Run k-mean clustering with i centers\n",
        "  cluster_KMeans = KMeans(n_clusters =i, random_state=0)\n",
        "  cluster_KMeans.fit(X)\n",
        "  #Store the summed inner cluster distances into the list namely 'dist'\n",
        "  dist.append(cluster_KMeans.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aRiilxo8bGJ"
      },
      "source": [
        "Display the inner cluster distances for all k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPTDSExDBY0v",
        "outputId": "38f29446-7c42-426e-868c-1b09a61bfca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4543749.614531862,\n",
              " 2370689.686782968,\n",
              " 1331903.0622637183,\n",
              " 916379.187153917,\n",
              " 647326.0020260847,\n",
              " 412137.50910045847,\n",
              " 324523.6250001953,\n",
              " 270954.92924153747,\n",
              " 217887.378560333]"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNud2CgJ8hYy"
      },
      "source": [
        "Now, let's use `matplotlib` library to visualize the inner cluster distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "dyljMec38NjX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MexfgXCq8o1Y"
      },
      "source": [
        "We can plot the inner cluster distance curve and use Elbow's method to seek for optimal k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "a8w02SxZ8P4j",
        "outputId": "94fb1a0f-6b8d-4085-9799-06c4695687e2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "elbowPlot = pd.DataFrame(dist)\n",
        "elbowPlot.rename(columns={0: \"Inner cluster distance\"}, inplace=True)\n",
        "elbowPlot[\"Number of Clusters\"] = np.arange(2, 11)\n",
        "\n",
        "plt.plot(elbowPlot[\"Number of Clusters\"], elbowPlot[\"Inner cluster distance\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DBSCAN algorithm is included in the cluster subdirectory of scikit-learn.\n",
        "\n",
        "To create the model, we need to decide our radius factor, `eps`, which tells us how large we think our clusters will be, and the minimum number of samples we want included in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DBSCAN(eps=40, min_samples=20)"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "cluster_DBSCAN = DBSCAN(eps=40, min_samples=20)\n",
        "cluster_DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the DBSCAN algorithm to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DBSCAN(eps=40, min_samples=20)"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cluster_DBSCAN.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict the clustering groups using our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "DBSCAN_predict = cluster_DBSCAN.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike k-means, DBSCAN tries to figure out the optimal number of clusters.\n",
        "\n",
        "How many clusters do we have in this fitted model? And what are they?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "(array([-1,  0,  1]), array([99, 42, 37]))\n"
          ]
        }
      ],
      "source": [
        "print(len(set(DBSCAN_predict)))\n",
        "print(np.unique(DBSCAN_predict, return_counts=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here we have two groups (0 and 1) with some outliers (-1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why are we seeing different results using k-means and DBSCAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use TSNE, another dimension reduction algorithm besides Principal Component Analysis, to visualize our data in two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fbbc0b03670>"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABdLElEQVR4nO3dd3hUVfrA8e87fdIh9N5774KgYAGRJmLvolhXXX/q6uq6lrXtWta29t4riIqKUkQEpEjvoRNqEkibybR7fn9MEjKZSa8k5/M8eUzu3Ln3TAzvnDnlfUUphaZpmlb3mWq6AZqmaVr10AFf0zStntABX9M0rZ7QAV/TNK2e0AFf0zStnrDUdAOK06hRI9WuXbuaboamadpJY9WqVSlKqcaRHqvVAb9du3asXLmyppuhaZp20hCRPUU9VilDOiLytogcEZENRTx+uoiki8ia3K8HK+O+mqZpWulVVg//XeAl4P1izvlNKTWhku6naZqmlVGl9PCVUouAtMq4lqZpmlY1qnOVzikislZEfhCRnkWdJCIzRGSliKw8evRoNTZP0zStbquugP8n0FYp1Rd4EZhV1IlKqdeVUoOUUoMaN4440axpmlbr7NuazPrfNuPKdNd0U4pULat0lFIZBb6fIyL/E5FGSqmU6ri/pmlaVTl2+Dj/N/ohkrcfAAQRuOLBC7jsgWk13bQw1dLDF5FmIiK53w/JvW9qddxb0zStKt024gH2bUnGCCiMgEHAb/DuPz9j0ZdLa7ppYSprWeYnwFKgq4jsF5HpInKjiNyYe8o0YIOIrAVeAC5WOi+zpmknub2b93No5+HwBxS88bcPq79BJaiUIR2l1CUlPP4SwWWbmqZpdcaWFUlFPpZ28Fg1tqR0dC4dTdO0csg6ns2a+RH3mgLQoFlC9TWmlGp1agVN07TayJvjZUaf/yPlQBHbjwSufazYgY8aoQO+pmlaGT13w2sc3R953YnJbOKiv01mzCUjq7lVJdNDOpqmaWXgynQz/+PFER8zmU1c/9TldB/ahS+f+5bV89dTm9an6B6+pmlaGWxbuQOzxYQRMMIeU0rx4aNf4s7KyX/cEW3n8Tn303tk9+puahjdw9c0TSuDmAbRmEyRQ6dSiux0V8ibQU62h3vOepiU5JrfeqQDvqZpWhl07NuOJm0aEdxKeoKIYDJLxOf4fQG+f+OXamhd8XTA1zRNKwMR4fEf7qdl5xbYo+04YxxY7VYuvHsSZrM58pMUHNgRukErEAhU+/i+HsPXNE0ro2btmvD25v+StHoXmWlZdB3cEWeskx/fWUD60Yyw881WM31yx/D//GUdL9/+Nns3JxMV52Tq7edy+YPTin6zqES6h69pmlYOIkLnAR0YcGYfouOD4/r/+OxOTObwsJrYvCFjLhvJluXbeXDKU+zdnAyAK8PNF8/M5tU736uWNuuAr2naSc+d5ebgzsN4Pb4abUff03vyftJLDB7Xj+h4J7ENo5l86zheWfUUzmgHHzzyJR6XN+Q5HpeXOW/8QnaGq8rbp4d0NE076ezffpCZz3/PzvV7cWW42L1hX/7KmMQWDXjg07/S69SaWQbZtG1jHp9zf8TH9m7aH/G42WohZX8q0T2iqrJpuoevadrJ4/Ceo/znmpeZ3vMOvn11Lht+28zOtXtClkGmHjjG3Wc+wq71e2qwpZF16Ns2bHUPgDvTzXM3vMaf89ZX6f11wNc07aSwcu5apve8g7nvLcTwGyij6BUufq+fj5+YWY2tK50rHrwAm9MW8bGNv2/lwclP8usXVZdHXwd8TdNqvUAgwJOXPx82/l2cXetqXw+/U//2PDX3QboO7ggRevoel5dX/vpOlS3X1AFf07Rab/eGfXhzyjYhKyZhWpNrOTfqUv5+zmPs3ZJcRa0rm57Du/LSH08SFeuM+Hh6SibZ6VUzgasDvqZptZ7NYcUoZginMDEJydsPkZ6SiTfHx8q5a7jtlL8Xnc64BjQsIl++2WLGEW2vknvqgK9pWq3XqksLmrRORCLMeIpJsEfZg48JtOrSHLPFhK/AEk2lIDvdxSMXPIM3p/TDQkVRSrHh9y18/fz3/D5rOX6fv8zXuPTv52OPCg3s9igbE286G4u1ahZQ6mWZmqbVeiLC2OljePOe0DqxzlgnLy1/gjZdW+Yf+2POnzxx2fP4veHDIluWbecfk57iqbn/KHdbvDle7jvnMbat3EHAb2CxmYmKdfLfxf+iWbsmpb7OmVeM4tiR43zw8Bf4PD4CfgObw0bTto0IBAJVsvNW9/A1Tav1clwePnz4i7DjRsBgzbzQMoMtOzfH543c41ZKsXHJVpJW7yp3Wz59ahZblieRk+3B5/Hhzszh2KHjPHH582W6jogwatopmK1mAv7gstLMtCxe+et73ND3LjYu2Vrpk7c64GuaVuttXZ4UMWWBx+Vh4ee/hxxr1bk5fUZ1j3g+gAjsWLu7VPf1eX0c2n2EHJcn/9hP7yzA6w4dFjIMxbaVO8lIyyzVdQEy0jK5sf/dZB8P/SRiBAz2bNrPPWc9wpNXvlipQV8HfE3Taj1HjKPIdffR8eG7U//51d10Htgh4vkiQrP2xQ+9KKX44pnZTGs8net738n5ja/lf3e8Q8AfIOAPFPm8vJ56acx++Udcme4iH/e6vSyZtZw/vv+z1NcsiQ74mqbVel0GdiAuMTZsl6oj2s6km8aGne+IsvPo7HtxxjpCjputZpq0aUSfUT2Kvd8vHyzivX9+jivTTU62B6/by5w35/H2/R9z+kUjsNrDpz/bdGtJgybxEa93/Gg6a3/dyKHdR/KPrfplfbGbxyBYPOWXjxYVe05ZVErAF5G3ReSIiGwo4nERkRdEJElE1onIgMq4r6Zp9YOI8Nicv9OgWQOiYp04Y51Y7Vam/nUCg8f1j/icBk3i+e9v/6LzgPaYLWYsVjODx/Xn6QUPhaz2CfgDZB7LwjCCvXOP28Or//cungLDOBAcPpr98k9c+vepNO/QFGdM8M3EHmUjOj6Kv73/l7A2GIbBS7e9xaVtbuKfU/7N9B538PdzH8ednUOz9k0QU+SCKQVV5uStVMb4kIiMArKA95VSvSI8Ph74CzAeGAo8r5QaWtJ1Bw0apFauXFnh9mmaVjcEAgHWLtxEZloWvU7tRmLzBqV6nivTjcVqxuY4kdbAMAyeuPwFfv18CcpQmC0mpv51ApuWbGXj71sjXsdiNfPlkbewOW0smbWCTcu20aJjU8ZcOpLYBjFh58966QfeuvejkDkAq93KaRecwvl/ncAdIx8odvewI9rOwzPvYcCZfUr1OgFEZJVSalDExyprQkBE2gHfFRHwXwMWKqU+yf15K3C6UupgcdfUAV/TtKry0Pn/5veZK8KOmywmjCLG4hs2S+DT5Ncj7geI5IqOt3Bo15Gw41a7hZlp77J8zmr+e+PreD0+fDk+DMMIbjILGIjJxLnXn8lNz11d6vtB8QG/utbhtwT2Ffh5f+6xsIAvIjOAGQBt2rSplsZpmla/+P1+fp8VHuyBIoM9wNhrR5cp+GYdz454XKlg3pyR5w9j+JTBJG8/RExCFFa7lcVf/4Erw82gcf1o271Vqe9VGrVu45VS6nXgdQj28Gu4OZqm1bBDu4/w58/rcMY6GTZxIM5oR8lPKkHaweNQxuhisZlp36ttmZ7Tf0wvFs9cHjY526hlQ2IbBoeATCYTbbqd2Dh2zvQzytawMqiugJ8MtC7wc6vcY5qmaUV658FP+fLp2YhJguvqb4B/fXtfiatsShLXKDaYrbIMQV/ERO9RZSuqMv2Jy/hz3no8Lg9+bwCTSbA6rNz8/DW8eOtbzH1vId4cL71GdOMvL19H+15VO6pRXcsyZwNX5q7WGQaklzR+r2la/bZu0Sa+evY7vDk+PC4v7swc3Jk5PDj5qXKXMkw9eIwHJj7BlPirigz2E244C5vDGnLM5rQx7f8m0KhFwzLdr2Wn5ryx7lkm3TyObkM6Meaykby49HG+efEHfnpnPh6XB2UoNizezB2nPlDlyd0qpYcvIp8ApwONRGQ/8E/ACqCUehWYQ3CFThLgAq6pjPtqmlZ3/fj2fLxuT9hxpRRr5m9gyDmRl2MWJeAPcMepD3B0X0rEDVJmi4nzbj+XG/5zJTc9dzVf/fd7ls5eQXzjOCbeOLbM98vTuFUiNz17df7PezbvZ8PiLSHpnpUCn8fHt//7iWv+dUm57lMalRLwlVLFtlAFlwLdUhn30jStfvDm+ChqEaGvHD385T+sJj0lIyzY26NsXPv4pUy59RxMpuCgh81h45J7z+OSe88r831Ksm9LMmarGQptsvV5/BXK8VMaeqetpmm10ukXDccRE54X3u8L0G9M2OrvEiVvP4jPE55UzePykrI/NT/YV0RKciqb/9hOdkbRBUzadG9FwBeensFqtxaZDqKy1LpVOpqmaQDDJw9m4Jl9WPXzOnKyPZgtZsxWM7e/cj3RceH5c0rSoU9brDYL/kKZNJ0xDjr1a1+qa6ycu5YPH/2CQ7uO0nVwR65+5CLa926LO8vNY5c+z+pf1mGxWfD7Alx492Su/OcFYcs423RrSZ/TerB24cb8YR2RYJGXSTeHp4moTJW28aoq6I1Xmla/+H1+dq3fizPWSavOzVFK8ecv61gyewUx8dGcdeVptOrSosTr+LzBQGq1nZh8NQyDG/rdxb6tB/J72GarmcYtE3lr83+x2a0Rr5Vn3se/8dyMV/N3xooI9igbz/32KJ8+OZMl36wMGWpyRNu5/ZUZnHn5qLBreXO8vPX3j/nxrfl43B76nNaTW164tlLW3VfLTtuqoAO+ptUfi2f+wTPTX8EIGAQCAVp0bMbDs+6hefumpb7G0f2pPHv9K/w5bz0A/U7vxZ1v3EiDpvE8eeWLLPt2FUop/F4/FquZ0y8ewQ1PX0lC48hJz/IYhsFFLWZw/Eh6yHER6HdGbzYs2hwxB3/73m14fe0zpW5/ZagNO201TTsJKKUwAgZmS+VXWyrOnk37ePKKF0Lyyuxav5ebBtzDZwdfx+4oucarz+vjtuH3k3bwGEYgODG7ZsEGbht+P0PH9+eP7/4M6YGbrRY69W9fYrAHSD+aEbGwuFKwbeWOInPvpx/NKPHa1UlP2mqahjvLzTPXvcK5UZdxjv0Sbht+PzvX7am2+3/76tyIE6rZ6S7+fs7jpbrGkm9Wkp2enR/sIVhMxJXh4uf3fw2rZetxefjque9Kde3ohOgiM1s2bpmIM9YZ8bFeI8u2Uauq6YCvaRoPTHySeR/9hs/jQynF5mXb+OvIf1T5RiCvx8eOtbvZv/1gSKAuaNOSrRzec7TEayVvPxgx82ROtodAEdfOPBY5101hNruVc6aPwe60hRy3R9m57IHzg2vnI7wf7Fizu9ybxKqCDviaVs/tWr+HrSuSwta2+7zBjUBVZc6bv3BBk+ncOerBsLq0BVkdVvZuKTkTS4c+bbFH2cKOO2MdNGiaEHZcBHqP6Fbq9t74zFWcdeVp2BxWHNF2omKdXP3oxaz6eS3PzXg14s7d1ANp/PrZklLfo6rpgK9p9dz+bQcjjtn7PH52VNGwztqFG/nfHe/iynTjynQXXzbQF6BVl+YlXnPwOf1o0roRFtuJqUmL1UzDZg24+52bsUfZMeUOy5gtZhwxDmY8fWWp22yxWrj9lRl8eeQtXl/3DF8efQu/18dP7y4s8jk52R5W/ry21PeoanrSVtPqubY9W+OPsBHI5rDSbXBHAJTygG8DSDRYupYpRXAknz/9TUhFqYDTTObARviaOHDsySJmTRoSUFgdFgae3bdUK3XMZjP/Xfwv3rz3Qxbm9qpHThvG9U9dTlzDWF5Y8hifPjWLvZv2021oZy66ZzLNO5R+BVAeZ4wTZ0xwzP7LZ78ttkyhxWqmSevEMt+jquhlmZqmcf+EJ1izYD1ed95GICE6IYq3Nz9PfOxiyHiA4CB1AExNkQavI5Z25b7fDf3uyp8U9jZzknxbT5RFUDYzZq+BpHvp+No2Jl52Otc9cVlIparSSklO5ZuXfyRpzW66DuzApFvG0bBZ6SpkldZY60VFzj1AcIz/jfXPlGlpaUUVtyxTD+lomsY/v/w/Jt9yDjENorHarQwZ358Xlz1BQoNDkH4fqGxQWaDcENiDSrsSpYoOdHnSc3KYt2sHy5P3YxToXA44q0/+0MvhyzpiOMwoW3BYKWAzIU2jGPTZFdz83DXlCvY71+1heo+/8tWz37HyxzV8/vRsru1+R6nmAsqiWbsmRT5mtVv451d3VWuwL4nu4WuaFtHxHDc79txBn9iFmE2F4oREIwmvIvaiS1O/vXoV/1nyG1azGaUg1mbjvSnT6JyYyLHDx5nR5/9Iz8kh6Z/9wBLe92zocLJyxs3lavsdpz7AxiWhdWlFYMCZfXjyp3+U65qRXNHxZg7tirCCSODT5NdJrORPFKWhe/iappVJhsfDhI8/ICVrd3iwB0BAHQOCSys/eXIm13S/nau7/IUPHvmCJTt388zSxXgCAbK8XrJ9Xg5nZ3HVrC8xlKJB0wReW/s046efUeR8gKWIzUx5NizezF9H/YPzEq/m5kF/4485fwLBXbGblm0LO18pWLtwU9l+ESXISI28rNNsMZeYqqEm6ElbTavHlPICAURCNw59sn4taW438w60YXiTfURbC22KUl6wDuDw3qPcNOAeMtOyTjz3yZlkpG0jp0NoKUIFZHo9rDqYzOAWrWjYrAF3PHstK7/+gmXJ+0KGfOxmMxd0D2bETE46yFv3fcSaBRuJSYjm/L+eS5vurfjHpCfz191vP7aTRy98hrvevpnTLhiO1W7F6w5fkx9p2WZFdOzblvW/bQ47Htsghuj4sid4q2q6h69p9ZAy0jCO3YI63A91eABGyvko34khkEV7d5MT8DN7Tyf2Zcfh9p/oGwaUA6KvxeuN56aBocEewJfjI8PnjVhQSkTI8oYG4qfPHkfzmFiirVbsZjNRVit9mjbjliFDSUlO5ZYh97J45nIy07I4uPMwb/ztIx6/9L9hm6w8Li+v3fUBQP56+YJsDivnXFe59WKve+ryCJuxbFz/78srJd1yZdM9fE2rZ5RSqLQrwL8LyO25+9ej0i6Fxj8jpoa0iI3FJILXsDBt3hQu7rCZ8a13kOmz8enOPmTQkrMzF+DOzIl4D+efR/F0jsdXaDjIFwgwsHlotstmMbEsuGo6i/bsZn9GOj2bNGFAsxaICO8++21+GcA8HpcnZElnQWkHj+Hz+Ljxmas4uPMwGxdvwWy14Pf56T+mF9c8enH5f3ER9BjWhf/Mf4i37/+YHWt307RNY6586EJOmRhxCL3G6UlbTatnlHcF6tj1oAonA3NAzC2YYm5g45HDXPDlp+T4w/Pb5LEaQrPHV2NLCQ/6JoeFnKdGkmL24fb7EcBhsXD38JFc3W9Aqdt669D72LoiKey4iBApdkXHRzEz7d38eYE9m/ezf+sB2vZoVaq0ynWBzpapadoJ/r1FPJAD/h0A9GzSlP+cOY77F/yMPxDAFSHwB0yKzLGtSPwoPCBbTWY+ufRy5h3eyw9J22jgcHJ57770b162oNu6Wwu2r9qBUWhzk5gEFCE9f0eUnQvvnhQyCdy2e6tKyTFfV+iAr2nl4DcMlu7fy/GcHAa3aEmzmNgaa4vy70Nl/gs8v4PYwHEeEndX2ERsPms3IheLdYK1b/5P53bpytkdO/Hdti08uHAe2b7QXDsG4G0Xi8lsCtl8ZLGZeWbhQzRMjOOCxF5c0KPs5QjzXHjXJH77alnYeH3YZieBc288m4uroAZtXaIDvqaV0fbUVC6f+QVunw+Fwm8YXNNvAHcPH1nhlANlpYx0VOo0UOmAEVw94/4c5d+KJH4Y8Tli7Ymy9QPvn0DeWLgZTDGIc3LIuVazmVPbtsPjC+/hi4LTBvWg9SWx/D7zD1Aw9NwB3PbK9cQ1rJw3wPa92/LwzHt47obXOLI3pcg0BiaTidQDqXhzfDiiSs6dX1/pMXxNKwOlFKe9+ybJmRkhq1CcFisvnjOBMe2rtgh1YUbW25D1X6DwOLoTSfwYsfaM+DylPKisF8H1JeAF+xgk9m7EHL4rNPNYFiP/7wkye8Tn74YFEG+A14aP48zhfSrt9RQlO8PFtMbXRsz5U5DVbmX0JSO47eXrsDvrZ+Cv8o1XIjJORLaKSJKI3Bvh8atF5KiIrMn9uq4y7qtpkWR4PDy37HfGffguF3zxCd9v2xpxgq88Nhw9QlqOO2zJodvv46P1ayrlHmXiX094sCe4rdS/vcinidgxxd6FqekyTE3/xJTwdMRgD7Dsu1W0mrmPuN+PIJ4AGArrQRct3tjGrtnrKumFFM+V4cZUiipcPo+PhZ/+ztPX/q8aWnXyqfCQjoiYgZeBs4D9wAoRma2UKryl7TOl1K0VvZ+mFcfl8zH50w85lJWJJxDsDW4+epQ1hw5y/6jTK3x9t8+HqYhhm0xv+Eaf8lDKAN9qMNLB1h8xFbM939IN+IUTQzP5FwFz+0ppT8AXQPwGjb7ZQ+LsPWASJKAQkbAc+lUlsUUDouOcETdTFebN8fH7rBWkp2QQ3yiuGlp38qiMHv4QIEkptVMFt+19Ckwu4TmaViW+2rSBI9lZ+cEewOX38cH6NRzOyirmmaXTt2mziJ8WHBYLEzt3rfD1lX8P6ugZqGPTUel3oY6Mwsh8FeVPQvm2hCUsk6gLQOyElluygaUzWEsealFKobxrUDkLUEbk6laDz+mfP0kqCiQQfP12p41R004p1+ssK5PJxF9evj64U7YU0yRWm4XUA8eqvmEnmcoI+C2BfQV+3p97rLDzRWSdiHwpIq2LupiIzBCRlSKy8ujRksuaaVpBv+7ZjTvCEkKbycyawwcrfH27xcITZ5yNw2LBnNvTj7JY6dwwkQt79q7QtZVSqGPTwTgQXCOvsgAPZD+HSpmCSrsYdXQkyrsCgLRDx/ju9ZXM/+FWvIG+BP85W8FxLtLwnRInkJV/HyrlLNSxq1Hp/4c6chpG1ssoIwvlnoXKfh/l24bZYqJdrzahv4coG2dcMYqewyv+JldaI6cO5T/zHmLElCF06NOWU6cOpevgThFrzbqzc3DG1M8x/OJUeNJWRKYB45RS1+X+fAUwtODwjYgkAllKKY+I3ABcpJQaU9K19aStVlYPLviFTzasI1Do7zrKauX9KdMYUMZ14EXZnprKJxvWkeLKZkz7Dozv4MCqksHSGTE3K9c1lW9TcLdr2IaoQiSKxYse56mrPkKE/E8cVz40jQvvPq9UK4WUUqiU8RDYRXCBZZ68TwsmwI/CxK+zG/LvvzQl4Mvt5ZuEpm0b8972F2s8fUBy0kFm9L0rbKhHBLoO7sSLy56ooZbVnKqetE0GCvbYW+Uey6eUSlVK5Q0yvgkMrIT7alqYy/v0w2oOndwzi9AkOpr+zUouk1danRMTefC00Tw/djSTmjyNJW0K6vgdqKNnYaTfi1LFryaJyMikNP8klQqweeHTeHO8eNxevDk+vDk+Pnj4K3Zv3Ffi8wHwb8fwJxMa7CE4F5ADuAAvQg5DzzjEsDNPDPcoQ5F+NIOVP9V86b6WnZrTqV+7sONKwc71ezm483D1N6oWq4yAvwLoLCLtRcQGXAzMLniCiBT8lzYJCE8vp2mVoEtiI/47djzxdgfRVisOi4VujRrzwXkXVGiN/E87tjPh4/cZ+PrLXD3rKzYeCQYSlfEweJcDOaAyAQ+456Cy3yr7Tay9oRRvFIKHRs3Dh618Xj8LP/u9VLf6/vWZuDNLN8nsjDYYd0no+L7P42P3hqJ27FavnCLy6lisZjJSM6u5NbVbhVfpKKX8InIr8BNgBt5WSm0UkUeAlUqp2cBtIjKJYKamNODqit5X04pydsfOjGnfke2pKUTbbLSJT6jQ9T5ev5bHfluYPzewaO9uVhzYz+fnT6O7mgMUDpw54PoAYmaEHD2UlckzS3/n1927iLHbuLrvAC7v0y9/1Y+YolBx/4CMR3KvGbmiVHAEJ3woVilFwB/6HJ/Xh8lswlzgU8+mZdt4+8G1jB4b+dqR3hfN1tD7WR1WWnauvE9MFTHs3IHs25KMzxP6JqgMRfvebYp4Vv1UKQNwSqk5SqkuSqmOSqnHco89mBvsUUrdp5TqqZTqq5QarZTaUhn31bSiWEwmujduUuFg7zcM/rPkt7CJ4By/n5eWLwKK6JGr0BVBx9xuJn7yAbO2bCLF7SIl6wg7kl9h4Ya/YLi/w/AloVQOpqhpSOJH4JwC1v5EWpIiAp16ha84sjmsjJo2DICtK3dw06B7mBB1GRNjruCZ617BnR1crz//49/ISvfz0t9bkuMS8hY05biESFULc1wm5n99ohC32WImPjGOYRNqx8js+XdOIKFJfH46ZJFgLdmbnru6XOUR6zKdWkHTipHiyg5Z4plHASsPHQNzOwjsKPSoCWyhyxU/XL+GLK+XgFJ0jkvj0zHfYDUFiLIEUOlzEQSFHRU9HYm5DVP8kyjvWlTaVQTH00M179gcm8NGwB9AGQZWu5XJt4yjy8COHNp9hLvHPIQ7KxjgDY+PeR/9xuE9R/n3zw8GPwUoxS9fNmT3VgeTrkmhWRsvaYctpB62MuHKFMQkWG0KpeyIvT8ZmZ0wmTciIgwZ3587Xp2BuRQboapDXMNYXlvzNLP/9yN/fP8njVomMvWOc+k1oltNN63W0QFfq9NWHzzAf/9YwvbUVDo1bMgdw0aUaaVOgsMRafQEgOaxcUj8o6i06QSHYAKAFcSBxN4Tcu7y5P35bxzPDZtHrNVL3mrC4H8UkAPZb6NMiUj05WDtBSYnGIUDvpPGnWbw2tqh/Pr5EvxePyPOG0KnfsGNVrNenBM2vOHz+Ni0ZCt7tyQz+uIR/PLBr+Rke0haH0XaEQunTzmOMsAICEqZ+PnTBNzZZvwM5bJHnuGxOULAH2x/bQn0BcU2iOGy+6dx2f3TaroptZoO+FqdtXTfXqZ/OzM/p/uh7CxWHjzA6xOmcGqbtqW6hsNi5aJevfls4/qQ3PAOs5kz23fkzgVHaGS7g6s6r6eF8zDY+iFRV4alKWif0IBl+/eRYMumfexxIiwdz+WG7Nch+nJEzJDwIurYdbkD9x7ADvYR4JhIq85mLrv//LAr7Fq/F3+EZGdmq4UDSYcYeu4Azr7qdH56dwHdB6Qx5bpU7I68d7Xgf0efd5yL+vRk2MQW+ZPdtTHQa2WjA75WZz3628KwAh45fj+PLlrAT5dfXerr3D/ydATh043rUAqcVgtdExvx2qoVuP3B1AKfbGnHxM7jeOLMsyNe4+p+A/hq80ZUabaJFtjxKrZB0PhXyPkBjGNgGwrW/sWuOOo2pDPrf9sc1sv3e3207dkKEeEvL13H2GtGY8r+Gw5n+MC9CAwe4+HMy0eV3F7tpFH7ii5qWiXZlpoS8fj2tNQyJVOzmEw8eNpoVs+4hV+vns77U6ax9vCh/GAPwRw+s7dtZv2RyOu+OzRoyBsTzyPK1pRt6YkEjGICv7VHyI9iimdL9hg+3T2CBckJYZvKCpt0yzjsTnvIm4LdaWPohIE0b3/ik0eXgR3p0Lt5xFU5CPQc0YlTJtXOUn1a+egevlZnNXA4SXWHT3g2cDjKtSbfbrHQxBLD15s34TfCe8WeQIBfd++id5PIWSeHt27Dr1dfR1rWqYjr2hOpE/IJYEdiTySc9RsGt8z5lt/27gaCm8hibHY+m3ZRkSuQEps34MVlj/Pq/73HmvkbcMQ4mHjj2Vz2QPjwjzgnoDyLAHfIcWe0lQvue7TGd9JqlUsHfK3OumHgYJ5b9nvIkkqnxcL1AwZX6LrRNhsWkyks6FtNJqJtxS8DFBESY7ujYn6FnJ9RvpXgXQtGKli6IbG3IwV6+B+uW8PivbtDhqbcfj+3zvmW2ZdcUeR9WnVpwb++va/kF2M/Kzgn4F2Sm9LBAlgwJTyKyVxzVby0qqEDvlZnTe8/kOM5ObyzZlV+0eur+vZnxsCKBfxzOnXhicW/hh0XESZ07ory70Zlvw+BnWAdiERfhpgaFjrXBs5zEee5xd7r4/Vrw/YAGEqxLS2Vw1lZNI2JqdBrETFBwsvgXYLK+QVMcYhzKmIp3aS2dnLRAV+rs0SEu4afyq1DhnI4K5sm0dE4rdYKX7dRVBQvnTOR2378Ln+XbMAweG7seBpZN6JSrgd8gB+8q1CuD6HRTMRc9sRtXiPyxi6TCN4C+wO2rtzBu//4lJ1rd9Oic3OueuhC+o0uXS1ZEQH7CMQ+oszt004uusShppWT2+djyb69KBTDW7fFabGgUs6CQOEcMyZwTMSU8J8y3+OZJYt5c/XKsM1frePiWXjVdESEjUu28rezH8VTIKeM3Wnj3g9v49TzhpbnpWknseKyZeoevlbvKd8mlOsrwIXYzwb7aRx1uVi0Zzd2i4XR7ToQE2Fs3mm1ckaHjieuY6RBIFLOfQM8i8rVthkDBzN3ZxLJmRm4fD7sZjNmk4nnxo7Pn3h+7a73QoI9gMft5ZU732XElCHVXlhdq710wNfqNSP7fch8mrxkZSrnB/a4ujN+zhDMJjNCcM/TK+dOYmTbdiVczVH0Q6Zo4ESJRLuldP/0Yu12vr3kCn5M2sYfyftpHRfP+d170jg6Ov+cnWv3RHxuyv40vDneelvMWwunA75Wb6lAKmT+h5ClkcpFI8tahjdpzIKDJyYub5ozmz+m31jsKhwxRaHso8GzgOAYfh4nR9T53PjZR2w8chgR4fR27XlizNkkRkWV2E6b2cykrt2Z1LV7xMcTmsZzeHd4dTh7lA2rveJzFlrdoRfZavWXdymR+jzRVj/jWu0MOSYIC3fvKvGSEv94MK89DpAYwIbHehbnfB1g/eFDBJTCbxgs3L2LS77+vEwbwIpy2f3nY48K7cXbo+xMvf1cvY5eC6F7+Fr9JfbgXqdCMddvCC5/aM9YofAEwvPThF3SFIckforybYNAMli78cGa/XiMJSG38RsGBzMz+CN5P8NaFVniuVTGXTuG9JQMPn7s62DpQkMx8aazueKfF1ToulrdowO+Vicp5QX/DjA1DEtkls8+kkj55n2GmS93hxbnDhhG2Bj+kewsDKVoFhO+QUmsXcDaBYCktLVhOX0guJ5+b/rxCgd8EeHiv53H1DsmkHbwGA2axutxey0iHfC1OsdwfQaZTwZ/UD6UbRCS8Dxiig85T8QBDV5DHbsBULkZKQP8dHgcuzJbAMEJVpvZzN3DR9I4KjhRuvNYGrf9+B1JaWkI0CounhfGnUv3xk0itqdvs+Z8v31r2AYqgG6NGlfa67bZrTRrF7kNmgZ6Hb5WxyjPUtSxGwnNDWMF20BMDd+P/ByVE1w2qXLANhxMiSzZv5efkrbjsFiY2r1nfmD2+P2c+s7rpLndIUM0sTY7v11zPXH28J51ttfLmR+8TYrLlZ/4zG420795Cz6eemElvXJNC9Lr8LV6Q2W/SeFEYOAD72pU4EDE3a4iDnCEpjUe0botI1qHpxf4eWcSOX5/WE0UvxHg221buKx337DnRNtsfHPx5Ty5eBHzdu3AZjYzrUcvbh96Sti5JVFK8cuHi/jkiZkcO3ScbkM7c92Tl9Gxb7syX0urf3TA1+oWI3J6YsQKRgqUI71BQQezMkNSGuRx+/0cyMwo8nlNomN4duz4Ut9HKYU7Kwd7lC2kAPnn//mGDx75Mn+j1cqf1rBh8WZeXPYE7XpWbC5Aq/v0mi2tbrGdCkRaex4AS+dSX0YZmSj3HJT7e5RxIpD3bdocqzm88lOU1Ur/Zs3L0eBwv339B5e2vYmpiVczJeEq3rz3QwL+AN4cLx8++mXEXbXvP/x5pdxbq9t0D1+rUyR6Oso9C1QGkDdJ6oSYOxFxluoahvsnSL8bJDewKz8q/glMzgkMbtGS3k2asvbwofyVN3azmfYJDRjdrkOF279mwQaeuuIFPG4vAAG/h1kv/YA3x8ekm8dGTJOgDMXW5UkVvrdW9+kevlaniLkx0mg2RF0C5g5gG4Y0eAFT9FWler4KpED6XUAOqOzgFx5Ivw8VOISI8O7k87ll8FDaxifQOi6e6wcM5rNpF2OuhE1OHzzyRX6wz+Nxefn+9V9wxjrx+yNnz2zRsVmF763VfZXSwxeRccDzgBl4Uyn1ZKHH7cD7wEAgFbhIKbW7Mu6taQBK+cE9E+X+ElDgmIo0ug+RMv6J5/xIpLX5oCBnDkRfi91i4ZbBw7hl8LBKaHmo5O2HIh43mQWv28uZl49i/ke/hbwp2KNsEatZaVphFQ74ImIGXgbOAvYDK0RktlJqU4HTpgPHlFKdRORi4CngooreW9MgOMGpjt8KnqXkr9DxbUV5foYGb5YqW6QKHAIjJXe8PtKO2kBw2WYV6zSgPWkH0yi8WlpESGzZkL+8NB2bw8oPb81HGQaxDWK4+flrS537XqvfKqOHPwRIUkrtBBCRT4HJQMGAPxl4KPf7L4GXRERUbd4EoJ08fGtCgz0Ev/etAt8KsA0p8qnKyEIdvx28f4DYQHmLONMK9tMrr81FuPrhi1gzf0NobvsoO5fePxVbbiK0W1+Yzoz/XIk7001cYqxOf6yVWmWM4bcE9hX4eX/usYjnKKX8QDqQGOliIjJDRFaKyMqjR8MzAGpaYSpnLhCh963cwUBe3HPT7849x5tbVNxLMLmOjeDQjgBOcE4LqTVbVTr1b8/T8/9J75HdccQ4aN6xKX95cToX3TMl5Dyb3Up8ozgd7LUyqXWrdJRSrwOvQ3CnbQ03R6vljMxnwfUeYRnQAHCAKdivUEqFBUdlpIHnN4JBvqAAmNuDbRCgEOdEsFasDm5ZdBvSmWd/faTa7qfVH5UR8JOBgjs+WuUei3TOfgnOosUTnLzVtHJT3pW5wb6ILJZiYnlqbx78+l2S0lKJdzi4rv8gbhw0JFiL1jgOYok8jKPcmOLDg67yLEO53gMjFexnELBfzKY0N3azma6JjXSPW6vVKiPgrwA6i0h7goH9YuDSQufMBq4ClgLTgPl6/F6rKOWeWfREqsSRpB7nmm/n56+XP56Tw8srlpHp9fC3EaPA3JrI/wTMEKGgt5H9LmQ+R95cQcC7iQOH3uTGBReT6bOSGBXFGxOn0CWxUaW8Pk2rbBUew88dk78V+AnYDHyulNooIo+IyKTc094CEkUkCbgTuLei99W0YM88Ur/BCXGP8PiyzLC0xG6/n/fWrsbl8yFihdj7CS1NaAGJQWJuDr2VkQWZz1JwYtgsXho5spjYZg0uv499Gelc9vXn+CKkXtC02qBSxvCVUnOAOYWOPVjg+xxAV2PQKpU4JwSXXipXoUcMxD6S7WmfRHyeSYTD2Vm0T2iAKeo8lKUlKusNCBwIbtSKuR4xF9rI5NsQzMdT6BOF0xLgjBZ7eHNrPwA8/gCL9uxmTPu24F0MgcNg7YdYQ/Prl5XH7+fzjev5ZtsW7GYzl/bqy/jOXfQQklYmtW7SVtNKzTYK7GeC5+fgihwsgBni/oWYYuma2IiDWZlhTzOUoll0TP7PYhuCNCx66SYApgQizRUYBqTknEjZEFAKt2cX6ug1oDJBBXv7yj4KSfhv2TeCEayOddnXn7Mp5Wj+J5Y1hw6xZP8eHhtzdgnP1rQTdGoF7aQlIkj8f5AGb0LUdIi+BWn0PaaoyQDcPvQUHJbQAOu0WLi67wCc1jIW97Z0BVNLCv+TyTEsvLe9d/7PhlKc0fAFMI7kpmXICX55FqFcH5fjVcIvO3ewJTUlZHjK7ffx9eZN7Dp+rFzX1OonHfC1k5qIILbBmOL+hin2FsTSBoCDmZmkut08cvoZdG/UGJMIDZ1Obh86nLuGn1q++zR8CyydACdKYsgJWHlm/XBWpgSzZDotFq7q3Ry72gkYha6QA65Py/Uaf9u7C5fPF3bcJMLy/fsiPEPTItNDOvXEvvR0vty8gRSXi9Ht2jO6XYdKSfZV2wQMg7/P/5nZWzdjM5vxGwY9Gzfhzxk3E2d3lHyBYoi5OSR+C/7tiEpn4+IAc5//HGfvbEx+RYM/jjL67x0ouh/lKeJ48RpFRWM1mfAZoW8iZjHRwFm6DKCaBrrEYb3w844kbv/pewKGgc8wiLJa6d2kKe9PmRYxt3t18QYCrDqQjIFiUPOW2C2R+x9KKfCtBu/y4Fi6Yzxiiot47turV/HM0sUh9WOtJhNj2nfglXMnV1rbs45nc0nrG8jJDg3iNqeVWUn7MUvhJGg2iL4WU+ydZb7X/ox0zv7w3bAVRw0cDpZce0ORvzetftIlDuuhY243Ly1fxg9J2zicnRWyeNHl87Hu8CFmbtnEhT17F3mNijCU4uvNG/l0wzpMIlzQozdTunXPf4NZum8vN82ZjVGgw/HCuAmc3q59yHWUCqCO/wU8vwPeYL6bzCehwduIbUDYfd9fuzqsWLjPMJi/aycun4+oUo7dKyM7OBlspIFtKGLtGfL44pnLi3ie4tcfpzJm/Lug/LltjgJTcyR6RqnuXViruHheOmcid86dg2EoDBQJDgdvTDxPB3utTPRfSx3k8vmY/NmHHM7KChsGyOP2+5m1dXOVBHxDKc756D22p53YTL3y4AEeWPAz/z5rHKe3bc91387C7Q8dl755zmx+vfo6GkdFnziY801usM9d/66C/1XHb4HGixEJ/YSS6Y2c/ExEcJcy4CvfelTa1bkrbHyABWU/HUl4DpHgcI0rw0XAH/679Xn87N7WGLlyLsr9NQT2Ibah4BiHiK3EexdlTPsOrLjuJjYcOYzdYqF7o8Z6SaZWZnVvEFdj5pZNpLrcRQb7PA5z8P1+W2oKv+/bw/GcwsW/y+eDtatDgn0en2Fw7y8/8eqq5UTaMKWU4rttW0OPub4ivCg5wfXw/o1hh0e1bRdMm1BIs5hYGpZivFspA3Xs5uCSSlwEA74bPAuDbz65Bp7dFzGF38cRbWfo+AGIuQmmmBsxxT+GOCdVKNjnsZrN9G/egh6Nm+hgr5WL7uHXQcuT94X1ngtzmC2M79KFSZ98wI5jaVhMJryBADcOHMLtw4ZX6P6fbFhf5GOeQICFu3ZFfDPyBgJk5BROlVDMHFOE+ae7hp/Koj27cfm8eAIBzCLYzGaePOPs0gVJ/5bcYF+YG+X6AnGeB0Db7q0YP30MP76zIH8c3xFtZ+i5A+h1areS76NpNUAH/DqobXwCNrMZbzFb/J1WCx+tXcOWlKP4CwTO1/9cQffGjTm7Y+kLfhdmjtDzLSigDCwmE/5CQd9hsXJq27Yhx8Q5DeXbSFgvXxxgDS/60TI2jrmXX81H69eyPHk/HRo04Op+A+jQoGH+OcrIAu/vwR9sIxBTTIErBIhc8SrvsRNufv5ahk4YxKofvsIZlUnXYRMZfO7puvet1Vo64NdBF/fqw9ur/8RL0QHf5fOx4egRAoV6yW6/n7dWr6pQwL+m3wDu+eWniI9ZTSbO6tiJo9nZbDm8jLEtN+Ew+1l0qDOxMSMY0KxF6BOck8EzF7xLQXkAO4ggCS+Gjd/nSYyK4rahp0R8LLxAuYGKfxqT86zgz5YewXuQHfpEcYLjvEIXS2XAgIcY0GdnbtbNXyD7eoi9rcjfjabVJB3w66AWsXG8M2Uqd839kUNZWfiM8MDvCQSK7MemuSs2lj+tRy++2LSBFQdCs2QLEGe3c02/gTRUXxPI+BKUHxGDKzpvx+JUwMTQ54gZEl4B30rwrgBTA3Ccg5gSytwuFTicW6DcEzpSlH4nyjY/WABdzJDwPOr4DbmTtp7gKhtrXyTqRN1YpQKoY7eCfyvgP3G97LdQ1q6IY2yZ26dpVU0H/DrmmNvN6kMHSXQ6WXDltaw8mMxVs77GEwjPAyMiFN6HYTWZObNDxwq347NpF7N4726eXrKYfRnpOCxWzurQkZsHDyXR7kIdfQqzePNHT0x4cnvy56EsbcG/GywdEHOz4BCJbXDwqyJyfiziAQk+Fn1F8Cf7UGg0D3K+QxkpiG0Y2E5BxIQKpKAyHgTPAgoP8QS5Udnv6ICv1Uo64NchLy5fyv9W/IHNbCagFE2io3l/yjTaxMeTlJYa0ql1WixM6daDmVs24fH7UYDdbCbB4eT6ARH3bJTZqW3acWqbdmHHlfvn4JBK4TlX5UKl3w/G0dz6sh6UYxwS/2RI0rG1hw7y9NLFbD56lNbx8dwxdDinFVq/H5FyEblYij8s46aYEyH6qpBPQUoFUGkXB7NqFjNchnG85LZoWg3QAb+OWLh7F6+uXI4nEMCTO1m7Nz2d62bP5NUJk7nkq8/I9vowlAquk+/UhUdHn8mFPXvzzupVHMjKZFSbtlzRpz/xjoqlIChZXr3YCIyDQCB3vB7ImYsyt4bo6eBdzM5jR7nxuwMcdgf/dNNy3Nw0ZzZPnTmWiV1KWB1jPw2yXiE8WFuCj5XEsyhY6aqoCltAsNj56JKvpWk1QAf8OuLdtX+G7TA1lGJfRjr+gMHia2bw297dHM3OZlCLlvmrVvo2bcZ/x51boXsrI1j8W0wNSzwXCAZXVdQegcLBOAdc76Ky3wYx0TTgY945Af6+8jRm7w1OLOf4/Tz220ImdO5a7AoZsfZAOadCzsz8DVxIXoHyUiylDOwGVdxyVzuYEpCY60u+lqbVAB3w64jjRUy0mk0mMr0eLCYTo9t1qNR7qkAqKv1u8P4R/NncGol/CrH1LfZ5YoqBBi+gjt1GsKevCAb6yLtkg2mGg6dF5f7FPjboV1amNOOAKxYITjRneb3E2u3F3zvun+AYi8qZBZgQx2SwDS3NywVLl9wiKIXbaQ6WS3ROQaIuLdeEsqZVBx3w64ixnTqzLTWVnEKTs0opejZuEna+UqpC68WVUqi0yyGwh/whjsBO1LGroNGP4RWjChH7adBkMXjmB4dv7KNQx26KuHs2uCG8cKZIxYQ2Sby+pT8ANrO5VGkTRATspyD2yMs2i2U7BcytwL+LE29OFjA1QRp9i0jxbzaaVtN0aoU64oo+/WkeG5tf8MMkgsNi4aHTzwhJsLXzWBqXff05nV96jp7/e54H5v8cMdd6iXyrcsfbC41nKz/K9XmpLiGmWMQ5GYm6MLgaJ+6h4BALeevrrQTH+8P/TM1iEGUO3ttpsXBNvwFVnu5ZxIQ0/BicU0Figss1HeORxC91sNdOCrqHX0fE2Gx8e8kVfL5xPQt276RJVDRX9htA7yZN889JcbmY+vnHZHo8KIKbrLYcms+fW15leAtzcNdp9BWlG4sPJBN54tULgV3leg1i6wuJ36Cy3wH/ZrD2DpYwPDaDwm8sXsPC4iMdcFosXNm3P3cMrVg6iFK30RSLxD8C8Y9Uy/00rTLpfPgnoYOZmfy+bw8xNjunt2uHw1K6lL95yzbzVvGc02oH/x6yALspQLBzbAZJQBrNRsyNi72W8iehUs4jvKiHE2LvRmyDwDgElh6IOXxIqSyMjKfB9QHBcoEKxIlhn0Cq5T4SHA6dIljTCtD58OuQ/y5bwmurlmMWEyKCSeDtyVMZ2Lxlic/deORIfrA3i8Fjg37FaSm4KiYAKhWV+SKSUHwPViydUPZR4PmNYCAGsIDEgvsLVOZ/ctfae1FRFyGxD5R7zsAUdxfKMRrl/gYIII4JmG3DaKpz1mhamVRo0FNEGorIzyKyPfe/DYo4LyAia3K/ZlfknvXZ8uT9vPHnCjyBAC6/j2yfl0yvl+tmz8RXTKK0PL2bNMWeW4CkbcxxYqxFjN3nzClVeyThvxBzS7C4tykRnOeDpS34twNuUMHlmri/RLm/LN2LLOpetoGY4h8Jphu2n6ITlGlaOVR0luteYJ5SqjMwL/fnSNxKqX65X5MqeM9667ON68PK3AEElGJZcsnFrC/u1Qe7xYIADnPRuXRO9NiLJ2LFFHMDpiYLMDVZisT+FXxrCZ/IdYPrvVJdU9O0qlPRgD8ZyPuX/B4wpYLX04rh9vmKzA7vifBGUFhiVBRfX3gpw1u3Ic0TXXSm+fKuIzeyKbx8Ml8gpXzX1DSt0lR0DL+pUupg7veHgKZFnOcQkZUEu35PKqVmFXVBEZkBzABo06ZNBZtXt0zo0pVFe3eHLaN0e328uXoVm44e5bLefUmMiiryGh0aNOSD8y4AwEhZESz4EcICUdeUq33K1Iyic8zU3sUBtUXAMPgxaTvfb99KtM3KRT37MKhFyXMzmlZaJa7SEZFfgEi7aO4H3lNKJRQ495hSKmwcX0RaKqWSRaQDMB84Qym1o6TG6VU6oQKGwXXfzmLFgf24fL787Uh5/7WbzURZbcy+5HJaxsaVeD3l34tKuxSMTILvxWawD0MSXkakdCt/Qq5nHEcdOYXIQT8KU7M1Zb5mfREwDK6d/TWrDhzA5fcFh90sFm4ePJRbBg+r6eZpJ5EKrdJRSp1ZzIUPi0hzpdRBEWkOHCniGsm5/90pIguB/kCJAV8LZTaZeGvSeSzcvYu5O7bzY9J2Mrye/EEUTyCAz8jh6SWLeW7s+BKvJ5Y20HhBblKwQ8Gc7xGqSJWaxIJEg8oIf8zSEaWM4ISuWMHcvtomXpVyg/IhppLfBGvK/F07WXUwGOyB/H0SLy1fxoU9etM4Orr4C2haKVR0DH82cFXu91cB3xQ+QUQaSO42RBFpBIwANlXwvvWWSYQx7Ttwz4iREevWGkqxcPdOUl2uCM8OJ2JFHGcgUZdVLNiTW6wk5tbc3bIFOcAxHnV0JCrtIlTKFFTKWJQ/qUL3K4kyjmEcuwl1eBDqyDCMo+eifOuq9J7l9dOO7RF3PFtMJn7ft6cGWqTVRRUN+E8CZ4nIduDM3J8RkUEi8mbuOd2BlSKyFlhAcAxfB/wKKm6zVbrHw4i3X2fKpx+y+/ixamwVSNRVEHs/mJoRTCrWCeL/BdkvBPPcKxeQA4E9qNTLUWGJyCpHMNfPNcFPL/gAPwS2o9KuRAUOVck9KyLObscU4ROPIMTYbDXQIq0uqlDAV0qlKqXOUEp1VkqdqZRKyz2+Uil1Xe73S5RSvZVSfXP/+1ZlNLy+i7JaOaN9R2ymyHVdvUaADUePcOGXn5ZqBU9lERFMURdiarIIU7PNmBrPgcDB3HKBBSnAk1s5qgr41gXTGVOo16z8KNcnVXPPCriwZ29s5vD/lyaTMDJCERlNKw+dPO0k9uSZZ9OnaVMcFkv+hqqCDKVw+3zM21XD0yWBQ0RMfaz8YFTRcs3APorM9eOvfdNH3Ro15h+jRmM3W4ix2Yix2kiwO3h38vk6dYRWafRfUi2nAkfAnwTm1oildchjcXYHn19wCdtSU3j+jyX8kLQ97PneQIADmZnV1dyIxH4KKmdmWBlBELBWTjnFMNbuwTeUMA6wDqiae1bQJb36cG7nLizbvw+nxcqwVq2xRngj17Ty0gG/llIqgMp4ANzfgtiDOWlsw5AGLyCFJkW7JDZictfuLNqzm+xCE39Ws5k+TYvPTR/x/v79qOz/gXc5mJohMTcg9pHh5+X8hMp+C4xjYBuFxNwYnnjNPgbMHXNTLuTt4nWCYwxi7VrmtpWGWDpGyPVjBlMMEjWtSu5ZGeLsDs7u2Lmmm6HVUXpIp5ZS2W+A+3vACyoT8IB3GSrjXxHPH9O+I63j4kPGge1mCz0aN2FwGTfvKP8+VOpkcM+EwF7wLUcduxXD9VnIeUbWS6jj94BvTbAQivsTVOpklJEWcp6IBUn8CGJuB0s3sPZB4v6BxD9dpnaVVTDXz43BCWSJB8dEJPHrWr08U9Oqkk6PXEsZR04FI3xbg19ZsTRdg8kUvkony+vllZV/8M2WzZhMwrTuvbhh4OAyjwEb6fdCbmbKEBKDNFmGiA1lZKCOjCA8PbINoqdjRN/O/F07WLp/H02jYzi/e0+9llzTqoFOj3wyUllFHPfz0bqVXNEvvERfjM3G3cNHcvfw8KGXMvEsJ/JuWSM4GWrpCP6tILZgecIQXgzPYi6Y04Ttaam4fD7sZjMvLV/GW5POY2ir1hGuq2laddBDOrWVdQhGhDxkOzMT+O/y1VTpJzNzESmRlB/yqmGZGoOKlF5Z2J0RxdbUlPyNRHnpnG/78XuMWvyJUtPqOh3waymJu5fsgA1vIPi/yG8ILr+Fx9cOY2qbZQSO3YCR8STKX3Ja5DLfO3oGUHi3rA3soxFTMFWSWNqBtQfhHxIdvLypR8Q0ztk+L9tSddZMTaspekinlhJLB25eNoPTGi+if6PDbD3ekFl7OvPi8F+It3oxef3g/T24iajhm4htMKkuFw//Op+5O5NQSnFG+448fPoZxY6dJ2dk8OLypSzbv48m0THcOGgIY9qPRsXeDVnPBE9SPrCfjsQ/GdrGBq+gjt8B3lUgFsAKcQ+xO/s4weSpoZRSETcXaZpWPXTAr8WuGzyBW+ao/N7yowMX0dDuxmrKGxbxAT5U+r0EGs5l2hefkJyZgT93LOjnnUmsO3yIeVdeG3HiNjkzg3M/eZ9sr5eAUuzNSOcvP3zLPSNGclXfy1FRFwZX6ZgaRixsLqYGSMP3UIGjoNLB3BYRKxf3Ws+21BTcBXr5AjSNjqF9QsSiaJqmVQM9pFMDlFLsPJbGlpSjxY5pj27XgZfHT6RrYiNsZjNnt9pbINgXEDjC73tWkeLKzg/2EKyEle7JYe7OyEnK/rfiD1y5wT6P2+/n6SWL8fj9iNgQS6eIwb4gMTcOnpebUnla956Mad8xdwewhWirjQSHk1cnTNalCTWtBukefjVLSktlxnezOJyVhYgQZbXy/NhzOaV15GIvo9t1YHS7DgAYR7+HQHaEswySjmUXMW7uIyktNeK1l+3fhz/iG46w8/gxujdqHOGxkplNJl48ZwKbU46yInk/jaOjGdOug04RoGk1TP8LrEbeQIBLvvqMNLc7v/6Ty+fjum9nMe/Ka2gWE4tSih93bOfTDevwBAKc17U753XvGRz7jroMMp8ltOasBWzDaJ3QCodlfdhO22irlU4NEyO2p3lMDLsiZNP0GQEaFVM1q7S6N2pc7jeNyqCMdFT2a5DzUzBls/NyJOpCRPQHW61+0gG/HJTyoLLfDu5EBXCeh0RfS27a/yIt3L2THL8/rNhfQBl8tXkjtwwexgMLfuGbLZvzC2GsP3yIWVs38+F5F2CKugLl2wg5PwaLiKgAWNohCf9hTEICjaKi8RQYwzeLEG93cHaHThHbc8OgIaw+dDBkrN1mNjOyTTsaR53cm6SU4UKlToXAYfITt2U+gfKvRuKfqtG2aVpN0V2dMlLKQKVdBVmvBNPvBnZD1iuotKuCFZ2KcdTlChkvz+MNBDiUmcXOY2l8vXlTfrCH4Jj6+iOHWbh7FyJmTAn/QRrNQeIfRxI/QBJnIaaGWEwmvrjgEsZ17IzVZMZqMnFWh058fdGlRQ6ljGzTjn+MGk2MzUaU1YrNbGZU23alqpZV2yn3N7mF0wtm6XSDew7KrwuKaPWT7uGXlXdpbuHvgsMqOeDfgtf9G/OSm3E8J4dhrVrToUHoZGdROW2irFaGt2nDsv37iDSn6fL5WLR3N2d06AgQzJppCd+x2igqihfOmVCml3NRj7ZMa3UcI+cHRKyYoy+gHOVsax/fMsAdflws4FsPlrbV3iRNq2k64JeVbx2onLDDSrl5/Y+3eH3LIAJKoRRM7d6Df40+M39lSpfERpzdoRM/70zKH0ZxmC10bNCQszp0Yu6OJMwRIr7VZCLRWfExdQAVOAjKC+Y2gA+VOg1T4AAmfMGaJFmvobwroMG7J/eKGnMrgn/ehSeyVdE7iTWtjtMBv6xMTQEHEJrb3e23sDfTHjJp+s3WzYxs045xnU6ku3127Hi+3ryRj9avxRsIMLlrd67s2w+LycQZ7TtgNoWPsplNJs7v0bNCzVb+vajjt+UW/zCBKQGc5wXLDoZUhfKAd03wjc3Wt0L3rEnivBiV/SGhAd8cTAlhHVhTzdK0GqWzZZaRMrJRR08DlXHiGJDhtTPyu8tw+UPHQ/o1bcYXF1wSMZBHsuHIYa7/dhZZXg+CICI8O/YczmjfsfxtVn7U0dG5wb3gPEOkHjCAA4n7GxJ1WbnvWRsoz1JU+j1gZAABsPZEEp5HzGWvD6BpJ4vismXqgF8Oyrc1mFIgsB8At2rGFQuGsyYlPM+6SYQGDievnDuJQaXMS28oxfojh/EG/PRt2rzC6QiUZyHq+F9BRVrDL1B43ZBEI/HPIo7RFbpvbaCUCmb4FAdiblLTzdG0KqfTI5dAKcXy5P0s3b+XBIeTiV26kVjMOnSxdkUa/4AKHADASlN2ZLxCeG74YPBOdbu4+puvWHLtDOLsjhLbYxKhbzmqVBUpcASKXEFU+A3fHCwWEqG61clIRMASeVObptU39X5ZZsAwmPHdLKbPnsmLy5fx7yW/MerdN1iyb2+JzxVzC8TcApvZzHNjx+OwWDAVMdGpFHy/fVtlN790rP0ID+wRTwTrACTxE0R0X0DT6pp6H/C/2bqZJfv24vL7UECO34/b7+fqb77ih+1bw/LOK6XI8HgIFEpWP6Z9B36+4hqGtWwV8T7egJ9j7gjLBKuBWLuA/XTCUx4XZIeGX2FK/AgxN6+mlmmaVp0qFPBF5AIR2SgihohEHDPKPW+ciGwVkSQRubci96xsX23eGLLTNI/fMLhz7g/cN2/uiXM3bWDIm68y6I3/0e+1l3h+2ZKQ5GctY+O4fdhwoqzhC9ltZgvDarDakyQ8B7H3ALYizlCIRY9xa1pdVtEe/gZgKrCoqBNExAy8DJwD9AAuEZEeFbxvpSlqCAaClZpmb9vClpSj/Ji0nX8snEeq24XfMMj2+Xj9zxW8+MfSkOcMat6S4a3a4Cywu9VpsTKybVv6N6u5nrOIGVP0ZRD7AOE9fQvYBpaYFVPTtJNbhQZqlVKbgZI26AwBkpRSO3PP/RSYDGyqyL0rKsvr5VBWJpO6dOPPgwdx+yOV6wuO8f++by9fbFoflo3S7ffz5uqV3DJkGJbcZZciwivnTuKbrZv5fOMGRODCHr2Z1LVbiRuZFuzeybNLf2dv+nE6NUzk7uEjK/1TgURdiPJvAPes3Jq0AbC0QeKfrdT7aJpW+1THzFxLoGAdvv3A0KJOFpEZwAyANm0qf3WFPxDg4UUL+GLjeqxmMwGlaBkbx+7jxyLmubGazcTb7RzIzIx4PW8gwAt/LGXn8TR6N2nGRT17keBwMrV7T6Z2j7xZyhsI8MvOJPamp9OzcRNGtGnLj0nbuOvnH/PfVFYfOsi1s7/m9QlTOLVN5aUBEDEh8f9CxdwMvo1gbgaWXif3rtpCtqQc5d+//8bqQwdpFBXFTYOGcF63HnXqNWpaeZQY8EXkFyDSGsH7lVLfVHaDlFKvA69DcB1+ZV77q00b+OfC+fnJyby5E68HMjM4v3tPvt6yKaSASJ6xHTvz6cb1/HnwQNhjfsPg9VXL8RoG83ft5LVVy5l54WW0TUiI2IbkzAymff4xWV4vHr8fu8VCu4QGpLpdYZ8gcvx+nlj8K99femUFX3k4MbcAc4tKv25N25GWyrQvPsHtC07Cp3ty+MeCXziYlcktg4fVdPM0rUaVOIavlDpTKdUrwldpg30yUHBcolXusWr1047gGLwrwtCN2+9n/u6dvDZhMjE2W8hXnyZNueDLT4mz2bEX2gBlEkFx4o0jx+8nw+Ph4V/nF9mOe37+kSPZ2WT7fPiVItvnY2tKCoeysiKev+NYWvlfdD30/B9Lw1JQu/1+/rdiOTlFDNtpWn1RHUM6K4DOItKeYKC/GLi0Gu6bb8ORwzy44JeIFaHyHM/JYXS7Dqy47iZWHTzApqNHeGbpYv5I3o8i2HO0mEx0a9SYfenpNI+NZUeESlKGUvy+L3L6XZfPl3+9gvzFpFVucpLnpa9uaw8filg20iSwPyOjyGIwmlYfVCjgi8h5wItAY+B7EVmjlBorIi2AN5VS45VSfhG5FfgJMANvK6U2VrjlpWAoxR0/fs+8XTsiLr0sKK8yk4jQuWEif583F08gkP94QCkCgWAlqDmXXolSiu7/ex5vgXPyFEyFkJSWygfr1pCckcHQlq2KrGErgMNiCWmn02Lh9qHDy/KS67228fHsy0gPO+43DJpE6zdPrX6r6CqdmcDMCMcPAOML/DwHmFORe5XH7K1bmLdrZ4nB3m42c//I03l+2RLeWL2SgGGEBPuC/jwQHMcXEc7q0JGfkpJCeug2s5nzcydr5+3cwV9+/A5fIEBAKZbuL3737s2Dh/LayhV4jQBOi4U7hg2vcJbM+ubWIaew8uCBkE9zDouFczt3LVVaC02ry+r0/vnPN60vcrklBHvV3Ro15qkzx/Lbnt28/ueKEt8c4h0OAobB3+f/zM87d2DkDtCYRLCZTPRp1py/jRiF3zC4+5cfQwJPcdduEx/PLYOHccPAIWR6PMTZ7aXOsKmdMKRlK545axwP/bqAdE8OApzfvSf/GHXyJ4LTtIqq0wG/cPqDglrGxnHfqaMY37krSikun/lFicHeabEwvf9A3lu7mu+2bQkZzhFgYPOWfDD1AgC2pqZEHO7JO7fgwI7dbOa+U08DwGIy0cBZXAoErSTndO7KuE5dOJbjJtpqK7LEo6bVN3X6X8LU7j3ZcORwWCBv4HCw4Krp+Zul/IZBpic802WeWJsNbyDAJb36cHW/AYx+762wawaUYvmB/bh9PvyGgcvrLfINp3V8PAFDcTArkzbx8dwzfBRnd+wc8VytfESEhpVUJUzT6oo6HfDP796TH7ZvY+XBZFw+H3ZzMJvli+dMzA/2ENxc1SI2juTMjLBrdElM5KkzxtIuoQHxjuAYcKa36DeHm+fMZsm+fZhECCiFSSRkotZpsfLXYSOY3LV7Jb5STdO0ktXpgO83ApzXrTut4uJw+330bNyUSV270yhCrvsHRp7OX+fOCZvse3DUGPoWyoEzqk07vtu+NWzFjUmE3/fuDZnEzVt9YzGZ8AUCXNSzN5O6dKvcF6ppmlYKdTbgJ6WlctGXn+EN+PEGAljNZo5mu7i8T7+I54/t1JnXbJN5dunv7Ek/TueGidw1/FQGtwhPd3z3iJH8tnc3Lp8PTyCAWQSryYyBirimfmjLVlzUsw/9mjWjWUxsZb9UTdO0UqmzAf/mOd9yPMedPznqMwxWHkzm/bWruW5A5EzOI9u0Y2SbdiVeu2VsHHMvv4YP1q1h5YFkOjRoQJ+mzXh00YKwiVoFZHq9IYXMNU3TakKdDPjJGRnsS08P29Ga4/fzxaYNRQb8skiMiuKOYSc2RaW6XPxjwS9h59nMZoYWURRF0zStOtXJhd4BZVBUYsRAMWkMKiIxKorLevfFaTlR/MQsQpTVytX9BlTJPTVN08qiTvbwW8fF0yQqmr2FttjbzRbO61p1tVfuH3k6nRsm8taaP0nPyWFkm7b89ZQRNNb5cDRNqwWkcM3W2mTQoEFq5cqV5XruusOHuHzmF/gNgxy/nyirlc4NE/nk/AtxWMJLEGqaptUFIrJKKRVx3LpO9fCVUixP3s/ifXtIcDj5+sJLWbp/HwczMxnYogWnt22v0xVomlZv1ZmAHzAMbpnzLYv37cndZGXmmaWLeWX8JK4oYilmWR3KyuTZpb/z047tZPt8RFmtXNyzD3eeMrxWfmpQSpHp9eK0WLAWyuWvaVr9U2cC/vfbt7J47578Aid52S5v+/F7Vlx/U0jK4vLYn5HOhI8/IKPALtssr5d316xi49HDfDT1wgpdv7L9lLSdRxYt4KgrG4vJxCW9+nDviFE68GtaPVZnxje+3rwpYjUrQxmsOlDxAlvPLVsSMaWCXylWHzzAhiOHK3yP0lJKoVTkxGwAy5P389e5cziYlZk/h/HJhnX8c+G8amujpmm1T50J+GZT5HWY2T5fkeUDy2LJvj1h6/rzibAl5WiF71ESpfwYmc+hjgxAHe6BkXIuyvNH2HkvLl8asT7uzC2byCgmSZymaXVbnQn4F/TojVUiv5z/rQwPimVVXOZFAdolNKjwPUqiMh6G7HdAZQMK/NtRx65H+UILiO05fjzi8y0mEymu7Cpvp6ZptVOdCfhjO3YiyhZ54jQ5M7jztiw2HjnMm3+u5MtNG8j0eLh+wGAcEfKqC9A+oQEDm7coT7NLTRnp4J4J5BR6xIPK+l/Ikd5Nm2KKsPPMUNAiVufy0bT6qs5M2ooITaJjSI8wZCGAzyh6zLsgQyn+b+4PzN2xHb+hsJpNPPzrfN6dfD7XDxjMa6uW4zcMDKUQ4OyOnXjyjLFIUVt7K0vgAIgNlLfQA8GefkG3DR3Owt27wurj3jhocK1cTaRpWvWoMwEfYEq37ry4fFnY+HUDp5P2pRxymbN9Kz/vSMoPlnlvFDd+P5tl029gev+B7EhLpYHDSav4+JC8+lXK3ApUpHKNJrCE7h7umtiIz6ddzBOLF7HuyCESnVHcPGgI03r0qp62appWK9WpgH913wH8tCOJpLRUXD4fDosFswgvjptQ6h74W3+uirjaJ8fvZ93hQ/Rv3oL+VTx8E4mYYlFRF4Prc8Bd4BE7EnNT2Pk9mzTlw9xyi5qmaVDHAr7TauWrCy5hwe6dLE/eT7OYWKZ0617qUnc/70hi/dHIyytFgmUMq0Omx8OfBw8QbbMxoHmL/PF4ib0PZWoCrrfBSAdrLyT2fsTatVrapWnaya1CAV9ELgAeAroDQ5RSERPfiMhuIBMIAP6i8jxUBrPJxJkdOnFmh05lep6hFA8s+DmsilX+dUXo27RZZTSxWB+tX8tjixZiMZtQShFrs/PulPPpktgIERMScz3EXF/l7dA0re6p6AD0BmAqsKgU545WSvWrymBfWh6/H5cvdNjmcFYWmd7CE6InvHDOhCrfpbr28CEe+20hOQE/WV5vcA9BdhZXzvqyyILomqZppVWhHr5SajNQ9StUKkmqy8V98+aycM8ulFL0aNyEp84cS7dGjYm124vs3Xdo0KBUlbAq6uP1a8MqZgFke32sOJDMsFatq7wNmqbVXdW1Dl8Bc0VklYjMKO5EEZkhIitFZOXRoxXfvXowM5N96ekYhsElX3/Owj278BsGAaVYf+QwF335KakuFzE2G2d16Ii9UC/eabFw08ChFW5HaRxzuyO+6YhAhqfw+ntN07SyKbGHLyK/AJEGr+9XSn1TyvucqpRKFpEmwM8iskUpFXEYSCn1OvA6BPPhl/L6YfYcP84tP3zLjrRURIRYm51MTw7+QkMjvoDBl5s2cMOgITx5xlhu9X7Hsv17sZnNeAMBruk3gKndq65oSkFnd+zE7/v24i60SsgXCEQspq5pmlYWJQZ8pdSZFb2JUio5979HRGQmMITSjfuXiy8Q4KIvPyXF7crvMRdem58nJ+An6VgaANE2G+9MnsqBzAwOZmXSuWEicXZHVTUzzMQu3fhw/Vq2p6bg9vsRwGGxcNvQU2jgdFZbOzRNq5uqfFmmiEQDJqVUZu73ZwOPVOU9f92zi2yft8gx+YKcFiv9Cq2+aREbR4vYuHLfX/k2QSAZrD0Rc+nX7NstFj47/yJmbd3MnO1bSXA4uKx3P4boIuiaplWCii7LPA94EWgMfC8ia5RSY0WkBfCmUmo80BSYmTuxawE+Vkr9WMF2F+tQVlbY0E0eE5D3iEWEOLuNKd0qZ8hGGcdQadMhsAMwg/KinBORuMeQIhK7FWa3WLioZ28u6tm7UtqkaZqWp6KrdGYCMyMcPwCMz/1+J9C3Ivcpq/7NmkdcORRltTKidRtWHzqINxDgjPYd+duIkUTbbJVyX3X8bvBvAQoMH7m/R1l6INFXVMo9St0WIws8P4ORDfZTEUu7ar2/pmm1T53aaZunZ5OmjGzTlt/27skfu7ebzbSKi+fFcyZiM5vZnHKUmZs38uIfy+jeuDFNoqPp1aQpzWLKl01SGZngXUpIsAcgB1wfQDUGfOVZhjp+Y3BtFAHIfAoVdRkS+7eTZgmtpmmVr04GfICXx0/iw3Vr+GTDOryBABO7dGPGwMHYzGbeXr2Kp5cuxuP35xc1sZhMmES4oEcvHjn9jLIHRuUmmJcz0mPVl4NeKS/q+M2gXKEPuD4B+yiwD6+2tmiaVrvU2YBvMZm4ut8Aru43IOT40exs/rPkt/yat3nyxvy/3ryRXo2bcFGvPmW7oalx8MsoXE7RAvbRZW1++XmXFvGAG+X+CtEBX9PqrTpTAKW0Fu3djbmYlMZuv593164u83VFBIl/EnBy4n3UDqYEJOa2crW1XIqpdRs5vbKmafVFne3hR6KUYs/xY3iLWJOfp7x1X8U+FBrNRrk+AP8usA1Boi5GTPHlul652IZGDvoShTgnVl87NE2rdepNwFdKcWdeJati1udbTCbO7NCx3PcRS1sk7oFyP7+ixBSNin8c0u8jNzkpiBNso8B+Ro21S9O0mldvAv7v+/aGVLKKxGG2EOewc+uQYdXYsspncp6LsvZF5cwGIxNxjAbrYL1CR9PquXoT8H9I2hqxkpXVZKJv0+Y4rRaGt2rDJb37VGs6haoillZIzM013QxN02qRehPw7WZLyC7bPDazhav69ufcLrpqlKZpdVu9WaUztXtPbJbw9zeF4vR27WugRZqmadWr3gT8Xk2acvvQU7CbzTgtFqKtVqIsVl4ZP6nSUitomqbVZvVmSAfghoFDmNy1O4v27MZhsTCmfUdidLDXNK2eqFcBH6BZTCwX6kyUmqbVQ/VmSEfTNK2+0wFf0zStntABX9M0rZ7QAV/TNK2e0AFf0zStnhBVikLfNUVEjgJ7arodxWgEpNR0I8roZGuzbm/V0u2tetXd5rZKqcaRHqjVAb+2E5GVSqlBNd2OsjjZ2qzbW7V0e6tebWqzHtLRNE2rJ3TA1zRNqyd0wK+Y12u6AeVwsrVZt7dq6fZWvVrTZj2Gr2maVk/oHr6maVo9oQO+pmlaPaEDfjmIyAUislFEDBEZVOB4OxFxi8ia3K9Xa7KdeYpqb+5j94lIkohsFZGxNdXGoojIQyKSXOB3Or6m2xSJiIzL/R0mici9Nd2e0hCR3SKyPvf3urKm21OYiLwtIkdEZEOBYw1F5GcR2Z773wY12caCimhvrfr71QG/fDYAU4FFER7boZTql/t1YzW3qygR2ysiPYCLgZ7AOOB/ImKu/uaV6LkCv9M5Nd2YwnJ/Zy8D5wA9gEtyf7cng9G5v9dasU68kHcJ/l0WdC8wTynVGZiX+3Nt8S7h7YVa9PerA345KKU2K6W21nQ7SquY9k4GPlVKeZRSu4AkYEj1tq5OGAIkKaV2KqW8wKcEf7daBSilFgFphQ5PBt7L/f49YEp1tqk4RbS3VtEBv/K1F5HVIvKriIys6caUoCWwr8DP+3OP1Ta3isi63I/MteYjfAEny++xMAXMFZFVIjKjphtTSk2VUgdzvz8ENK3JxpRSrfn71QG/CCLyi4hsiPBVXM/tINBGKdUfuBP4WETianF7a4US2v4K0BHoR/D3+0xNtrWOOVUpNYDgUNQtIjKqphtUFiq4pry2ryuvVX+/9a7EYWkppc4sx3M8gCf3+1UisgPoAlT5hFh52gskA60L/Nwq91i1Km3bReQN4Lsqbk551IrfY1kppZJz/3tERGYSHJqKNC9VmxwWkeZKqYMi0hw4UtMNKo5S6nDe97Xh71f38CuRiDTOm/QUkQ5AZ2BnzbaqWLOBi0XELiLtCbZ3eQ23KUTuP+o85xGcgK5tVgCdRaS9iNgIToTPruE2FUtEokUkNu974Gxq5++2sNnAVbnfXwV8U4NtKVFt+/vVPfxyEJHzgBeBxsD3IrJGKTUWGAU8IiI+wABuVErV+CROUe1VSm0Ukc+BTYAfuEUpFajJtkbwbxHpR/Cj+27ghhptTQRKKb+I3Ar8BJiBt5VSG2u4WSVpCswUEQjGgY+VUj/WbJNCicgnwOlAIxHZD/wTeBL4XESmE0ydfmHNtTBUEe09vTb9/erUCpqmafWEHtLRNE2rJ3TA1zRNqyd0wNc0TasndMDXNE2rJ3TA1zRNqyd0wNc0TasndMDXNE2rJ/4fBVAiJQXoSDgAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
        "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=wine.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks like two of the wines are hard to differentiate, and the third is still not that different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's time for a competition. \n",
        "\n",
        "You should have gotten a dataset on forest fires in Portugal taken from `kaggle.com`. We are going to try and predict the amount of area burned using variables like wind, rain, day of the week, and month of the year.\n",
        "\n",
        "Try different algorithms for regression tasks and tune their hyperparameters if necessary. The people with the 5 lowest mean square errors will receive a prize!\n",
        "\n",
        "*Hint*: The outcome variable, area burned, is very right-skewed, or most values are small and just a few are larger. Fitting models using the log of the outcome may help. Then when you predict values, exponentiate them to get them on the original scale. Use the `np.log()` and `np.exp()` functions to take the natural log/raise to the power of e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH',\n",
              "       'wind', 'rain', 'area'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fires = pd.read_csv(\"forestfires.csv\")\n",
        "fires.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A description of each variable is below:\n",
        "\n",
        "*   `X` is the x-coordinate of the fire on a map of the Portugal park (not very important)\n",
        "*   `Y` is the y-coordinate of the fire on a map of the Portugal park (not very important)\n",
        "*   `month` is the month the fire occured in\n",
        "*   `day` is the day of the week the fire occured on\n",
        "*   `FFMC` stands for \"Fine Fuel Moisture Code\" and indicates the moisture levels among the small leaves in the forest.\n",
        "*   `DMC` stands for \"Duff Moisture Code\" and indicates the moisture levels among decomposed organic material.\n",
        "*   `DC` stands for \"Drought Code\" and indicates how dry the deeper soil is.\n",
        "*   `ISI` stands for \"Initial Spread Index\" and takes into account the moisture of fuels for fire and windspeed to determine how likely things are to be spread around in the forest.\n",
        "*   `temp` is the temperature in Celsius during the fire.\n",
        "*   `RH` is the relative humidity in percentage terms.\n",
        "*   `wind` is the windspeed in $km/h$ during the fire.\n",
        "*   `rain` is the amount of rain in $mm/m^2$ during the fire.\n",
        "*   `area`, our outcome variable, is the amount of area burned by the fire in hectares.\n",
        "\n",
        "*Note*: a lot of these variables are correlated with one another. So it may be better to choose a subset of them when fitting models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then `scikit-learn` needs our outcome variable in a separate dataframe from our predictors. We create those two objects now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = fires[\"area\"]\n",
        "X = fires.drop(columns=\"area\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create our training and testing datasets. By making `random_state=0` we assure that everyone uses the same training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3,  random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK take it away! Have fun creating your models!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OHoqoS6zCuES",
        "iIQhfBLiK0Zn",
        "CjRNjZ5WK4pO",
        "KX7SnsiDWQ43",
        "6e5r4Un57I2-",
        "QTHEycO9LFI2"
      ],
      "name": "Python_Workshop_Session_3_Applied_Machine_Learning.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
