{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWTYa1f3ptN"
      },
      "source": [
        "# Introduction to Pyspark Tutorial\n",
        "\n",
        "Authors: Yusen He & Martin Pollack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W42ofBJWjhBJ"
      },
      "source": [
        "## Setting up Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxoHfY1F32ah"
      },
      "source": [
        "First we have to install the Pyspark package to our environment before we can even import.\n",
        "\n",
        "To do so we actually have to step outside of Python for a second. Instead we have to write a terminal comand to install a Python package. If you're not familiar with a computer's terminal, it is basically a place where you can talk directly to your computer.\n",
        "\n",
        "To exit Python and communicate with the terminal in a Jupyter notebook, type a `!` as the first character of the code cell. Then we run the `pip install pyspark` command in the terminal to install the PySpark package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD_GCSzwQle7",
        "outputId": "35b24a2b-f97e-4f40-f8dc-ef4d474b88c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can import objects and functions from the PySpark package as we normally do.\n",
        "\n",
        "We can then start using the package by creating a Spark session. This means we are substantiating a Spark Driver that communicates with us humans, doing so through a SparkContext. \n",
        "\n",
        "Note that for this example the Spark cluster we are connecting to will only include one node, the computer you are using. But normally our cluster would have lots of nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "wAUPr3l8yydi",
        "outputId": "411a7261-a38c-40f3-d58f-da24e34ced2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4d7d2f59446e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Dataframe</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc2cd659150>"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "SparkConnection = SparkSession.builder.appName('Dataframe').getOrCreate()\n",
        "SparkConnection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REzO33324Boy"
      },
      "source": [
        "Next we read in/load our data into our Spark cluster.\n",
        "\n",
        "To do so, we take our `SparkConnection` and use its `read` method. We specify some options to make sure everything goes smoothly and then give a `csv` file path.\n",
        "\n",
        "An important thing in Spark is the \"schema\" of the data. This just means the data types of the various columns. By telling Spark to `inferSchema`, we just let it makes its best guess on what the data types of our columns are.\n",
        "\n",
        "We can see that it does a pretty good job of doing so. `Name` was read as a string, and the others as an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqqubbHpRChC",
        "outputId": "336aa467-aecd-4fcf-f59a-b4a3bd0b202b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark = SparkConnection.read.option('header','true').csv('salary.csv',inferSchema=True)\n",
        "df_pyspark.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zftM3KUYi5B9"
      },
      "source": [
        "A connection to our data stored in our cluster is stored in `df_pyspark`.\n",
        "\n",
        "To actually see the data, we can use the `show()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMf02CBai4UC",
        "outputId": "b06f58fd-44a0-489f-92da-2def20ee7ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOB4RvOfkRSg"
      },
      "source": [
        "## Manipulating Data in Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3ivyqglOXo"
      },
      "source": [
        "Getting rid of an unneeded column is easy. Just use the `drop()` method.\n",
        "\n",
        "But notice that the column is not permanently dropped. If we want to have data without the `Name` column, we would have to assign our line of code to a new name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH-5Oa9RlN78",
        "outputId": "f6af8f7f-601f-4e34-f824-27010aba8fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+------+\n",
            "| age|Experience|Salary|\n",
            "+----+----------+------+\n",
            "|  31|        10| 30000|\n",
            "|  30|         8| 25000|\n",
            "|  29|         4| 20000|\n",
            "|  24|         3| 20000|\n",
            "|  21|         1| 15000|\n",
            "|  23|         2| 18000|\n",
            "|null|      null| 40000|\n",
            "|  34|        10| 38000|\n",
            "|  36|      null|  null|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Drop the name column\n",
        "df_pyspark.drop('Name').show()\n",
        "\n",
        "df_pyspark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead we can also specify the columns we want to keep. The `select()` method allows us to do this. Sounds a lot like SQL, doesn't it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pyspark.select([\"age\", \"Salary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz90R5zhpqjW"
      },
      "source": [
        "Selecting certain rows is just as easy with the `filter()` method. \n",
        "\n",
        "Just write out your conditional in a string, or surrounded by quotation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA1iZlcapvL_",
        "outputId": "333904d3-5229-49d6-f6c8-68e414c7ae2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter(\"Salary<=20000\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58CGcVvxqCE_"
      },
      "source": [
        "Instead of writing our conditional in a string, we can also use a \"boolean column\", or a column of trues and falses.\n",
        "\n",
        "Multiple conditions can also be chained together using \"and\" logic with the `&` symbol, exactly as we did in pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_zyFUeVqI4v",
        "outputId": "c0c146c9-a8e8-471d-ae0f-98bd62bd6d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter((df_pyspark['Salary']<=20000) & (df_pyspark['Salary']>=18000)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOy1kJdp1s_"
      },
      "source": [
        "We can also chain multiple of these commands together.\n",
        "\n",
        "For example, below we filter and then select certain columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lqDmnprp897",
        "outputId": "09aa605d-f40f-4e72-ee44-881c07ec95cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+\n",
            "|   Name|age|\n",
            "+-------+---+\n",
            "|  Sunny| 29|\n",
            "|   Paul| 24|\n",
            "| Harsha| 21|\n",
            "|Shubham| 23|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.filter(\"Salary<=20000\").select(['Name','age']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKPnMaNZooHn"
      },
      "source": [
        "Sometimes we want to drop all rows with missing values. This is done with `.na.drop()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbtdzalEor5Y",
        "outputId": "07ad41ec-2318-436a-e3f5-634699d13f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.na.drop().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZttiZRlQpN"
      },
      "source": [
        "Or maybe missing values are just code for a 0 or another value in our dataset.\n",
        "\n",
        "To subsitute a specific value for missing values use `na.fill()`, specifying first the value you want to put in, and then the columns where you want to make this substitution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOb9kewdlU-d",
        "outputId": "fbcc00da-014b-4d05-cca1-aacc043ecb5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "|   Mahesh|  0|         0| 40000|\n",
            "|     null| 34|        10| 38000|\n",
            "|     null| 36|         0|     0|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.na.fill(0,['age','Experience','Salary']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since for our later steps we cannot have missing values, we are going to remove them forever here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pyspark = df_pyspark.na.drop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojvaIRHrq4zW"
      },
      "source": [
        "## Pyspark Groupby and Aggregate Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWvKrbM3rSrW"
      },
      "source": [
        "Like in pandas and SQL, we may want to look at summary statistics by various groups.\n",
        "\n",
        "To do this in PySpark, we first call the `groupBy()` method with a column name. Then we say how we want to aggregate our groups together, or what summary statistic we are interested in.\n",
        "\n",
        "Here we look at sums."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOhUaI4YrXPF",
        "outputId": "7f6cb093-83f8-4fc9-e68a-f10b0227383c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----------+\n",
            "|     Name|sum(salary)|\n",
            "+---------+-----------+\n",
            "|Sudhanshu|      35000|\n",
            "|    Sunny|      12000|\n",
            "|    Krish|      19000|\n",
            "|   Mahesh|       7000|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Grouped to find the maximum salary\n",
        "df_pyspark.groupBy('Name').sum().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9A_jf_ErqN2"
      },
      "source": [
        "But we can also look at group means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqgrhBU2rw02",
        "outputId": "10451770-9341-4f39-9c77-adbb49865ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----------+\n",
            "| Departments|avg(salary)|\n",
            "+------------+-----------+\n",
            "|         IOT|     7500.0|\n",
            "|    Big Data|     3750.0|\n",
            "|Data Science|    10750.0|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.groupBy('Experience').mean().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbsml7BOr3qV"
      },
      "source": [
        "Or look at how many observations are in each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQtFT_Efr75V",
        "outputId": "1b60c8c6-0d34-4370-90e4-6178dd0160ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "| Departments|count|\n",
            "+------------+-----+\n",
            "|         IOT|    2|\n",
            "|    Big Data|    4|\n",
            "|Data Science|    4|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_pyspark.groupBy('Experience').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SQL-like queries in PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spark DataFrames are actually within \"Spark SQL.\" This means that we interact with a DataFrame and its functions, but really behind the scenes SQL is being run to interact with data in Spark.\n",
        "\n",
        "But we can also directly interact with our Spark data using SQL.\n",
        "\n",
        "First we create something called a \"temporary view\", and give it a name. This is basically a snapshot of our data that will be thought of as a relational database table we can interact with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pyspark.createOrReplaceTempView(\"SalaryTable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can just query from our SalaryTable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_df = SparkConnection.sql(\"SELECT * FROM SalaryTable\")\n",
        "sql_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Any SQL query, no matter how complicated, can be done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_highest_salary_df = SparkConnection.sql(\"SELECT Name, Salary FROM SalaryTable ORDER BY Salary LIMIT 1\")\n",
        "sql_highest_salary_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao52F020kV5p"
      },
      "source": [
        "## Machine Learning with Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1y0xYzKt6TG"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3o-rYhsdOO"
      },
      "source": [
        "To set up our data to do machine learning, we need to create one column with our outcome variable, and one column that for each row has a list, or a \"vector,\" of potential predictors.\n",
        "\n",
        "We create our column of vectors with the `VectorAssembler` object. We tell the object what predictors we want put into the vector with `inputCols` and the name of the resulting column with `outputCol`.\n",
        "\n",
        "Then we call the `transform()` method on our `VectorAssembler` object, giving our DataFrame as a parameter. This will actually give us the data we want, and we save the data to a new variable called `output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yckzTBquslHM",
        "outputId": "07a35f59-8270-4037-8731-8cb467e22649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---+----------+------+--------------------+\n",
            "|     Name|age|Experience|Salary|Independent Features|\n",
            "+---------+---+----------+------+--------------------+\n",
            "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
            "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
            "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
            "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
            "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
            "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
            "+---------+---+----------+------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "featureassembler = VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"Independent Features\")\n",
        "#Conduct transfrom operation on MLdata\n",
        "output = featureassembler.transform(df_pyspark)\n",
        "#Display the transformation\n",
        "output.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqalcxQs-R_"
      },
      "source": [
        "Now to do machine learning we only need our \"Independent Features\", or predictors, and \"Salary,\" or outcome, columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBCWpjYEtGlx",
        "outputId": "d6f69e46-86f1-44ea-e56e-78d41b423790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|Independent Features|Salary|\n",
            "+--------------------+------+\n",
            "|         [31.0,10.0]| 30000|\n",
            "|          [30.0,8.0]| 25000|\n",
            "|          [29.0,4.0]| 20000|\n",
            "|          [24.0,3.0]| 20000|\n",
            "|          [21.0,1.0]| 15000|\n",
            "|          [23.0,2.0]| 18000|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "finalized_data = output.select(\"Independent Features\",\"Salary\")\n",
        "finalized_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then create our training and testing datasets.\n",
        "\n",
        "This is done with the `randomSplit()` method of our DataFrame. We just tell the method the proportion of data we want in each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data,test_data = finalized_data.randomSplit([0.6,0.4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934Gvd-duAId"
      },
      "source": [
        "### Linear Regression in Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph0jc_SFtPum"
      },
      "source": [
        "PySpark has lots of machine learning algorithms in its `ml` sub-package. You'll soon see that the process for doing machine learning in Spark is very similar to what we did in scikit-learn.\n",
        "\n",
        "We start by importing the `LinearRegression` object from this sub-package.\n",
        "\n",
        "Like scikit-learn, we first specifiy the specifics of the model, like what column has our features and which has our labels or outcome. Then we use the `fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Sy522sZatpee"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "LRregressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
        "LRregressor = LRregressor.fit(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmeqdOF0tt-N"
      },
      "source": [
        "We can then use our model to make predictions on the testing dataset. We use the `evaluate()` method, then getting the actual prediction values with the `predictions` field of the resulting object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19gir_n4tyA1",
        "outputId": "5d130bda-864b-4116-b489-c0f3f480fd5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+----------+\n",
            "|Independent Features|Salary|prediction|\n",
            "+--------------------+------+----------+\n",
            "|          [23.0,2.0]| 18000|   15000.0|\n",
            "|          [24.0,3.0]| 20000|   15000.0|\n",
            "|          [29.0,4.0]| 20000|   15000.0|\n",
            "|          [30.0,8.0]| 25000|   15000.0|\n",
            "|         [31.0,10.0]| 30000|   15000.0|\n",
            "+--------------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Prediction\n",
        "LR_pred = LRregressor.evaluate(test_data)\n",
        "LR_pred.predictions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDV4myEwuD90"
      },
      "source": [
        "Evaluation metrics can also be calculated with, for example, the `meanAbsoluteError` and `meanSquaredError` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqx3A36euSlM",
        "outputId": "45b8a76d-4ae3-44ed-801d-dc4f3fb984c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The MAE is: 7600.0\n",
            "The MSE is: 76799999.99999999\n"
          ]
        }
      ],
      "source": [
        "print('The MAE is:',LR_pred.meanAbsoluteError)\n",
        "print('The MSE is:',LR_pred.meanSquaredError)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_alJ-faMuhBj"
      },
      "source": [
        "### XGBoost Regression in Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlV5O6hyurNN"
      },
      "source": [
        "But of course we can fit other models too. Next we make an XGBoost model.\n",
        "\n",
        "Below we import the model we want `GradientBoostedTrees`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "yqHRZajQu9JU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import GBTRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it's your turn!\n",
        "\n",
        "Create the `GBTRegressor` model and then fit it to your training dataset.\n",
        "\n",
        "Use your model to make predictions on the testing dataset, and find the mean absolute error."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "W42ofBJWjhBJ",
        "FxoHfY1F32ah",
        "fTTw_HsC37Za",
        "REzO33324Boy",
        "zOB4RvOfkRSg",
        "ojvaIRHrq4zW"
      ],
      "name": "PySpark for Beginners.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
